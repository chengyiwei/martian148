(window.webpackJsonp=window.webpackJsonp||[]).push([[25],{392:function(s,t,a){"use strict";a.r(t);var n=a(1),e=Object(n.a)({},(function(){var s=this,t=s._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("h1",{attrs:{id:"模型与分词器"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#模型与分词器"}},[s._v("#")]),s._v(" 模型与分词器")]),s._v(" "),t("h2",{attrs:{id:"模型"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#模型"}},[s._v("#")]),s._v(" 模型")]),s._v(" "),t("p",[s._v("可以直接使用对应的 "),t("code",[s._v("Model")]),s._v(" 类，或者使用 "),t("code",[s._v("AutoModel")]),s._v(" 根据 checkpoint 自动加载模型")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" BertModel\n\nmodel "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" BertModel"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-cased"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("p",[s._v("**在大部分情况下，我们都应该使用 "),t("code",[s._v("AutoModel")]),s._v(" 来加载模型。**这样如果我们想要使用另一个模型（比如把 BERT 换成 RoBERTa），只需修改 checkpoint，其他代码可以保持不变。")]),s._v(" "),t("h3",{attrs:{id:"加载模型"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#加载模型"}},[s._v("#")]),s._v(" 加载模型")]),s._v(" "),t("p",[s._v("所有储存在 HuggingFace Model Hub 上的模型 都可以通过 "),t("code",[s._v("Modelfrom_pretrained()")]),s._v(" 来加载权重")]),s._v(" "),t("p",[s._v("参数可以是和上面使用名字来加载模型，也可以使用本地路径来加载模型")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" BertModel\n\nmodel "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" BertModel"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./models/bert/"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("p",[t("code",[s._v("Model.from_pretrained()")]),s._v(" 会自动缓存下载的模型权重，默认保存到"),t("code",[s._v("~/.cache/huggingface/transformers")]),s._v("，我们也可以通过 HF_HOME 环境变量自定义缓存目录。")]),s._v(" "),t("p",[s._v("部分模型的 Hub 页面会包含很多文件，我们通常只需要下载模型对应的 "),t("code",[s._v("config.json")]),s._v(" 和 "),t("code",[s._v("pytorch_model.bin")]),s._v("，以及分词器对应的 "),t("code",[s._v("tokenizer.json")]),s._v("、"),t("code",[s._v("tokenizer_config.json")]),s._v(" 和 "),t("code",[s._v("vocab.txt")]),s._v("。")]),s._v(" "),t("h3",{attrs:{id:"保存模型"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#保存模型"}},[s._v("#")]),s._v(" 保存模型")]),s._v(" "),t("p",[s._v("保存模型通过调用 Model.save_pretrained() 函数实现，例如保存加载的 BERT 模型：")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoModel\n\nmodel "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoModel"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-cased"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("save_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./models/bert-base-cased/"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("p",[s._v("这会在保存路径下创建两个文件：")]),s._v(" "),t("ul",[t("li",[t("code",[s._v("config.json")]),s._v("：模型配置文件，存储模型结构参数，例如 Transformer 层数、特征空间维度等；")]),s._v(" "),t("li",[t("code",[s._v("pytorch_model.bin")]),s._v("：又称为 state dictionary，存储模型的权重。")])]),s._v(" "),t("p",[s._v("简单来说，配置文件记录模型的结构，模型权重记录模型的参数，这两个文件缺一不可。我们自己保存的模型同样通过 "),t("code",[s._v("Model.from_pretrained()")]),s._v(" 加载，只需要传递保存目录的路径。")]),s._v(" "),t("h2",{attrs:{id:"分词器"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#分词器"}},[s._v("#")]),s._v(" 分词器")]),s._v(" "),t("p",[s._v("由于神经网络模型不能直接处理文本，因此，我们要先把文本转成数字，这个过程被称为 Encoding，包含两个步骤：")]),s._v(" "),t("ol",[t("li",[s._v("使用分词器 tokenizer 将文本按词、子词，字符切分为 tokens")]),s._v(" "),t("li",[s._v("将所有的 token 映射到对应的 token ID")])]),s._v(" "),t("h3",{attrs:{id:"分词策略"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#分词策略"}},[s._v("#")]),s._v(" 分词策略")]),s._v(" "),t("p",[s._v("分词策略可以分为以下几种：")]),s._v(" "),t("ul",[t("li",[t("strong",[s._v("按照词切分（Word-based）")])])]),s._v(" "),t("p",[t("img",{attrs:{src:"https://pic-1301573324.cos.ap-chengdu.myqcloud.com/20250913103649.png",alt:"image.png"}})]),s._v(" "),t("p",[s._v("例如，直接利用 Python 的 "),t("code",[s._v("split()")]),s._v(" 函数按照空格进行分词")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("tokenized_text "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Jim Henson was a puppeteer"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("split"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenized_text"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("['Jim', 'Henson', 'was', 'a', 'puppeteer']\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("这种策略的问题是会将文本中所有出现过的独立片段都作为不同的 token，从而产生巨大的词表，例如 “dog” 和 “dogs”，如果给它们赋予不同的编号就无法表示出这种关联性")]),s._v(" "),t("p",[s._v("当遇到不在词表中的词时，分词器会使用一个专门的 "),t("code",[s._v("[UNK]")]),s._v(" token 来表示它是 unknown 的。显然，如果分词结果中包含很多 "),t("code",[s._v("[UNK]")]),s._v(" 就意味着丢失了很多文本信息，因此一个好的分词策略，应该尽可能不出现 unknown token。")]),s._v(" "),t("ul",[t("li",[s._v("按照字符切分")])]),s._v(" "),t("p",[t("img",{attrs:{src:"https://pic-1301573324.cos.ap-chengdu.myqcloud.com/20250913105324.png",alt:"image.png"}})]),s._v(" "),t("p",[s._v("这种策略把文本切分成一个一个字符，这样只会产生一个非常小的词表，并且很少会出现词表外的 tokens")]),s._v(" "),t("p",[s._v("但是从直觉上来看，字符本身并没有太大的意义，因此将文本切分为字符之后就会变得不容易理解。")]),s._v(" "),t("ul",[t("li",[t("strong",[s._v("按照子词切分")])])]),s._v(" "),t("p",[s._v('高频词直接保留，低频词被切分为更有意义的子词，例如 “annoyingly” 是一个低频词，可以切分为 "annoying" 和 "ly" 这两个子词不仅出现频率更高，而且词义可以得以保留')]),s._v(" "),t("p",[t("img",{attrs:{src:"https://pic-1301573324.cos.ap-chengdu.myqcloud.com/20250913105510.png",alt:"image.png"}})]),s._v(" "),t("p",[s._v("可以看到，“tokenization” 被切分为了 “token” 和 “ization”，不仅保留了语义，而且只用两个 token 就表示了一个长词。这种策略只用一个较小的词表就可以覆盖绝大部分文本，基本不会产生 unknown token。尤其对于土耳其语等黏着语，几乎所有的复杂长词都可以通过串联多个子词构成。")]),s._v(" "),t("h3",{attrs:{id:"加载与保存分词器"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#加载与保存分词器"}},[s._v("#")]),s._v(" 加载与保存分词器")]),s._v(" "),t("p",[s._v("分词器的加载与保存与模型相似，使用 "),t("code",[s._v("Tokenizer.from_pretrained()")]),s._v(" 和 "),t("code",[s._v("Tokenizer.save_pretrained()")]),s._v(" 函数。例如加载并保存 BERT 模型的分词器：")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" BertTokenizer\n\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" BertTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-cased"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ntokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("save_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./models/bert-base-cased/"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("p",[s._v("同样地，在大部分情况下我们都应该使用 "),t("code",[s._v("AutoTokenizer")]),s._v(" 来加载分词器：")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-cased"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ntokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("save_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./models/bert-base-cased/"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("p",[s._v("调用 "),t("code",[s._v("Tokenizer.save_pretrained()")]),s._v(" 函数会在保存路径下创建三个文件：")]),s._v(" "),t("ul",[t("li",[t("code",[s._v("special_tokens_map.json")]),s._v("：映射文件，里面包含 unknown token 等特殊字符的映射关系；")]),s._v(" "),t("li",[t("code",[s._v("tokenizer_config.json")]),s._v("：分词器配置文件，存储构建分词器需要的参数；")]),s._v(" "),t("li",[t("code",[s._v("vocab.txt")]),s._v("：词表，一行一个 token，行号就是对应的 token ID（从 0 开始）")])]),s._v(" "),t("h3",{attrs:{id:"编码与解码文本"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#编码与解码文本"}},[s._v("#")]),s._v(" 编码与解码文本")]),s._v(" "),t("p",[s._v("Encoding 过程包含两个步骤")]),s._v(" "),t("ol",[t("li",[s._v("分词：使用分词器按某种策略将文本切分为 tokens")]),s._v(" "),t("li",[s._v("映射：将 tokens 转化为对应的 token IDs")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-cased"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsequence "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Using a Transformer network is simple"')]),s._v("\ntokens "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tokenize"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequence"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("['using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("可以看到，BERT 分词器采用的是子词的分词策略，例如 “transformer” 会被切分为 “transform” 和 “##er”")]),s._v(" "),t("p",[s._v("然后，我们通过 "),t("code",[s._v("convert_tokens_to_ids()")]),s._v(" 将切分出的 tokens 转化成对应的 token IDs：")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("convert_tokens_to_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("[7993, 170, 13809, 23763, 2443, 1110, 3014]\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("我们还可以通过 "),t("code",[s._v("encode()")]),s._v(" 函数将两个步骤合并，并且 "),t("code",[s._v("encode()")]),s._v(" 会自动添加模型需要的特殊的 token，例如 BERT 分词器分别会在模型的首尾添加 "),t("code",[s._v("[CLS]")]),s._v(" 和 "),t("code",[s._v("[SEP]")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-cased"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsequence "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Using a Transformer network is simple"')]),s._v("\nsequence_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("encode"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequence"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequence_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("[101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("其中 101 和 102 就是 "),t("code",[s._v("[CLS]")]),s._v(" 和 "),t("code",[s._v("[SEP]")]),s._v(" 对应的 tokens IDs")]),s._v(" "),t("p",[s._v("在实际编码文本时，最常见的是直接使用分词器进行处理，这样不仅会返回分词后的 token IDs，还包含了模型需要的其他输入，例如 BERT 分词器还会在自动输入中添加 "),t("code",[s._v("token_type_ids")]),s._v(" 和 "),t("code",[s._v("attention_mask")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-cased"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ntokenized_text "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Using a Transformer network is simple"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenized_text"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], \n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], \n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("p",[s._v("文本解码 (Decoding) 与编码相反，负责将 token IDs 转换回原来的字符串。注意，解码过程不是简单地将 token IDs 映射回 tokens，还需要合并那些被分为多个 token 的单词。下面我们通过 "),t("code",[s._v("decode()")]),s._v(" 函数解码前面生成的 token IDs：")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-cased"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\ndecoded_string "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("decode"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7993")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("170")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("11303")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2443")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1110")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3014")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("decoded_string"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\ndecoded_string "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("decode"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("101")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7993")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("170")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("13809")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("23763")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2443")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1110")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3014")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("102")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("decoded_string"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("Using a transformer network is simple\n[CLS] Using a Transformer network is simple [SEP]\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br")])]),t("p",[s._v("解码文本是一个重要的步骤，在进行文本生成、翻译或者摘要等 Seq2Seq (Sequence-to-Sequence) 任务时都会调用这一函数。")]),s._v(" "),t("h2",{attrs:{id:"处理多段文本"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#处理多段文本"}},[s._v("#")]),s._v(" 处理多段文本")]),s._v(" "),t("p",[s._v("我们往往会同时处理多段文本，而且模型只接受 batch 数据作为输入，即使只有一段文本，也需要将它组成一个只包含一个样本的 batch")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" torch\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" AutoModelForSequenceClassification\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"distilbert-base-uncased-finetuned-sst-2-english"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoModelForSequenceClassification"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsequence "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"I\'ve been waiting for a HuggingFace course my whole life."')]),s._v("\n\ntokens "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tokenize"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequence"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("convert_tokens_to_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# input_ids = torch.tensor(ids), This line will fail.")]),s._v("\ninput_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tensor"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Input IDs:\\n"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" input_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\noutput "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("input_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Logits:\\n"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" output"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("Input IDs: \ntensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n          2026,  2878,  2166,  1012]])\nLogits: \ntensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br")])]),t("p",[s._v("这里我们通过 "),t("code",[s._v("[ids]")]),s._v(" 构建了一个只包含一段文本的 batch，更常见的是送入包含多段文本的 batch：")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("batched_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("实际场景中，我们应该直接使用分词器对文本进行处理")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" torch\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" AutoModelForSequenceClassification\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"distilbert-base-uncased-finetuned-sst-2-english"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoModelForSequenceClassification"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsequence "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"I\'ve been waiting for a HuggingFace course my whole life."')]),s._v("\n\ntokenized_inputs "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequence"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" return_tensors"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"pt"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Inputs Keys:\\n"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" tokenized_inputs"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keys"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nInput IDs:\\n"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" tokenized_inputs"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"input_ids"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\noutput "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("**")]),s._v("tokenized_inputs"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nLogits:\\n"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" output"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("Inputs Keys:\n dict_keys(['input_ids', 'attention_mask'])\n\nInput IDs:\ntensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n          2607,  2026,  2878,  2166,  1012,   102]])\n\nLogits:\ntensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br")])]),t("p",[s._v("可以看到，分词器输出的结果中不仅包含 token IDs（ "),t("code",[s._v("input_ids")]),s._v(" ），还会包含模型需要的其他输入项。前面我们之所以只输入 token IDs 模型也能正常运行，是因为它自动地补全了其他的输入项，例如 "),t("code",[s._v("attention_mask")]),s._v(" 等")]),s._v(" "),t("h3",{attrs:{id:"padding-操作"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#padding-操作"}},[s._v("#")]),s._v(" Padding 操作")]),s._v(" "),t("p",[s._v("按批输入多段文本产生的一个直接问题就是：batch 中的文本有长有短，而输入张量必须是严格的二维矩形，维度为 "),t("eq",[t("span",{staticClass:"katex"},[t("span",{staticClass:"katex-mathml"},[t("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[t("semantics",[t("mrow",[t("mo",{attrs:{stretchy:"false"}},[s._v("(")]),t("mtext",[s._v("batch size")]),t("mo",{attrs:{separator:"true"}},[s._v(",")]),t("mtext",[s._v("sequence length")]),t("mo",{attrs:{stretchy:"false"}},[s._v(")")])],1),t("annotation",{attrs:{encoding:"application/x-tex"}},[s._v("(\\text{batch size}, \\text{sequence length})")])],1)],1)],1),t("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[t("span",{staticClass:"base"},[t("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),t("span",{staticClass:"mopen"},[s._v("(")]),t("span",{staticClass:"mord text"},[t("span",{staticClass:"mord"},[s._v("batch size")])]),t("span",{staticClass:"mpunct"},[s._v(",")]),t("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),t("span",{staticClass:"mord text"},[t("span",{staticClass:"mord"},[s._v("sequence length")])]),t("span",{staticClass:"mclose"},[s._v(")")])])])])]),s._v("，即每一段文本编码后的 token IDs 数量必须一样多。例如下面的 ID 列表是无法转换为张量的：")],1),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("batched_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("p",[s._v("我们需要通过 Padding 操作，在短序列的结尾填充特殊的 padding token，使得 batch 中所有的序列都具有相同的长度")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("padding_id "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),s._v("\n\nbatched_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" padding_id"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br")])]),t("p",[s._v("模型的 padding token ID 可以通过其分词器的 "),t("code",[s._v("pad_token_id")]),s._v(" 属性获得。下面我们尝试将两段文本分别以独立以及 batch 的形式送入到模型中：")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" torch\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" AutoModelForSequenceClassification\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"distilbert-base-uncased-finetuned-sst-2-english"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoModelForSequenceClassification"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsequence1_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\nsequence2_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\nbatched_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("pad_token_id"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tensor"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequence1_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tensor"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequence2_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tensor"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("batched_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\ntensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\ntensor([[ 1.5694, -1.3895],\n        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("p",[s._v("我们发现了，使用 padding token 填充的序列的结果竟然与其单独送入模型时不同！")]),s._v(" "),t("p",[s._v("这是因为模型默认会编码输入序列中的所有 token 以建模完整的上下文，因此这里会将填充的 padding token 也一同编码进去，从而生成不同的语义表示。")]),s._v(" "),t("p",[s._v("因此，在进行 Padding 操作时，我们必须明确告知模型哪些 token 是我们填充的，它们不应该参与编码。这就需要使用到 Attention Mask 了")]),s._v(" "),t("h3",{attrs:{id:"attention-mask"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#attention-mask"}},[s._v("#")]),s._v(" Attention Mask")]),s._v(" "),t("p",[s._v("Attention Mask 是一个尺寸与 input IDs 完全相同，且仅由 0 和 1 组成的张量，0 表示对应位置的 token 是填充符，不参与计算。当然，一些特殊的模型结构也会借助 Attention Mask 来遮蔽掉指定的 tokens")]),s._v(" "),t("p",[s._v("对于上面的例子，如果我们通过 attention_mask 标出填充的 padding token 的位置，计算结果就不会有问题了")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" torch\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" AutoModelForSequenceClassification\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"distilbert-base-uncased-finetuned-sst-2-english"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoModelForSequenceClassification"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsequence1_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\nsequence2_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\nbatched_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("pad_token_id"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\nbatched_attention_masks "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tensor"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequence1_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tensor"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequence2_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\noutputs "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tensor"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("batched_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" \n    attention_mask"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tensor"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("batched_attention_masks"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("outputs"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\ntensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\ntensor([[ 1.5694, -1.3895],\n        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("p",[s._v("所以说，在实际使用的时候，我们应该直接使用分词对文本进行处理，它不仅会向 token 序列中添加模型需要的特殊字符（例如 "),t("code",[s._v("[CLS],[SEP]")]),s._v("），还会自动生成对应的 Attention Mask。")]),s._v(" "),t("p",[s._v("目前大部分 Transformer 模型只能接受长度不超过 512 或 1024 的 token 序列，因此对于长序列，有三种处理办法：")]),s._v(" "),t("ol",[t("li",[s._v("使用一个支持长文的 Transformer 模型，例如 Longformer 和 LED")]),s._v(" "),t("li",[s._v("设定最大长度，然后截断输入序列："),t("code",[s._v("sequence = sequence[:max_squence_lengeth]")])]),s._v(" "),t("li",[s._v("将长文切片为短的文本块，然后对每一个 chunk 编码")])]),s._v(" "),t("h3",{attrs:{id:"直接使用分词器"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#直接使用分词器"}},[s._v("#")]),s._v(" 直接使用分词器")]),s._v(" "),t("p",[s._v("正如上面所说，在实际使用时，我们应该直接使用分词器来进行分词")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"distilbert-base-uncased-finetuned-sst-2-english"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsequences "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"I\'ve been waiting for a HuggingFace course my whole life."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" \n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"So have I!"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\nmodel_inputs "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequences"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model_inputs"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("{'input_ids': [\n    [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], \n    [101, 2061, 2031, 1045, 999, 102]], \n 'attention_mask': [\n    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n    [1, 1, 1, 1, 1, 1]]\n}\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br")])]),t("p",[s._v("可以看到，分词器的输出包含了模型需要的所有输入项")]),s._v(" "),t("p",[s._v("对于 DistilBERT 模型，就是 "),t("code",[s._v("input_ids")]),s._v(" 和 "),t("code",[s._v("attention_mask")])]),s._v(" "),t("p",[s._v("Padding 操作通过 "),t("code",[s._v("padding")]),s._v(" 参数来控制")]),s._v(" "),t("ul",[t("li",[t("code",[s._v('padding="longest"')]),s._v(" 表示将序列填充到当前 batch 中最长序列的长度")]),s._v(" "),t("li",[t("code",[s._v('padding="max_lenth"')]),s._v(" 将所有序列填充到模型所能接受的最大长度，例如 BERT 模型就是 512")])]),s._v(" "),t("p",[t("strong",[s._v("截断操作")]),s._v(" 通过 "),t("code",[s._v("truncation")]),s._v(" 参数来控制，如果 "),t("code",[s._v("truncation=True")]),s._v("，那么大于模型最大接受长度的序列会被阶段，例如对于 BERT 模型就会截断长度超过 512 的序列，此外，也可以通过 "),t("code",[s._v("max_length")]),s._v(" 参数来控制截断长度")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"distilbert-base-uncased-finetuned-sst-2-english"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsequences "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"I\'ve been waiting for a HuggingFace course my whole life."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" \n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"So have I!"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\nmodel_inputs "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequences"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" max_length"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" truncation"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model_inputs"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("{'input_ids': [\n    [101, 1045, 1005, 2310, 2042, 3403, 2005, 102], \n    [101, 2061, 2031, 1045, 999, 102]], \n 'attention_mask': [\n    [1, 1, 1, 1, 1, 1, 1, 1], \n    [1, 1, 1, 1, 1, 1]]\n}\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br")])]),t("p",[s._v("分词器还可以通过 "),t("code",[s._v("return_tensors")]),s._v(" 参数指定返回的张量格式，设 "),t("code",[s._v("pt")]),s._v(" 返回 Pytorch 张量，"),t("code",[s._v("tf")]),s._v(" 返回 TensorFlow 张量，"),t("code",[s._v("np")]),s._v(" 则返回 NumPy 数组，例如：")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"distilbert-base-uncased-finetuned-sst-2-english"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsequences "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"I\'ve been waiting for a HuggingFace course my whole life."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" \n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"So have I!"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\nmodel_inputs "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequences"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" padding"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" return_tensors"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"pt"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model_inputs"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nmodel_inputs "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequences"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" padding"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" return_tensors"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"np"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model_inputs"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("{'input_ids': tensor([\n    [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n      2607,  2026,  2878,  2166,  1012,   102],\n    [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n         0,     0,     0,     0,     0,     0]]), \n 'attention_mask': tensor([\n    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n}\n\n{'input_ids': array([\n    [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n     12172,  2607,  2026,  2878,  2166,  1012,   102],\n    [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n         0,     0,     0,     0,     0,     0,     0]]), \n 'attention_mask': array([\n    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n}\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br")])]),t("p",[s._v("实际使用分词器时，我们通常会同时进行 padding 操作和截断操作，并设置返回格式为 Pytorch 张量，这样可以直接将分词结果送入模型")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" AutoModelForSequenceClassification\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"distilbert-base-uncased-finetuned-sst-2-english"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoModelForSequenceClassification"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsequences "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"I\'ve been waiting for a HuggingFace course my whole life."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" \n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"So have I!"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\ntokens "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sequences"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" padding"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" truncation"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" return_tensors"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"pt"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\noutput "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("**")]),s._v("tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("output"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("{'input_ids': tensor([\n    [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n      2607,  2026,  2878,  2166,  1012,   102],\n    [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n         0,     0,     0,     0,     0,     0]]), \n 'attention_mask': tensor([\n    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n\ntensor([[-1.5607,  1.6123],\n        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br")])]),t("p",[s._v("在设置了 "),t("code",[s._v("padding=True, truncation=True")]),s._v(" 设置下，同一个 batch 中的序列都会 padding 到相同的长度，并且大于模型最大接受长度的序列会被自动截断")]),s._v(" "),t("h3",{attrs:{id:"编码句子对"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#编码句子对"}},[s._v("#")]),s._v(" 编码句子对")]),s._v(" "),t("p",[s._v("对于 BERT 等包含“句子对”预训练任务的模型，他们的分词器都支持对句子对进行编码，例如：")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-uncased"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\ninputs "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"This is the first sentence."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"This is the second one."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("inputs"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\ntokens "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("convert_ids_to_tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("inputs"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"input_ids"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], \n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], \n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br")])]),t("p",[s._v("通过输出我们发现分词器会用 "),t("code",[s._v("[SEP]")]),s._v(' token 拼接两个句子，这是 BERT 模型预期的 "句子对" 输入格式')]),s._v(" "),t("p",[s._v("返回结果中除了前面我们介绍过的 "),t("code",[s._v("inputs_ids")]),s._v(" 和 "),t("code",[s._v("attention_mask")]),s._v(" 此外，还包含了一个 "),t("code",[s._v("token_type_ids")]),s._v(" 项，用于标记那些 token 属于第一个句子，哪些属于第二个句子")]),s._v(" "),t("p",[s._v("我们可以看到第一个句子 "),t("code",[s._v("[CLS] sentence1 [SEP]")]),s._v(" 所有 token 的 token ID 都为 0，而第二个句子 "),t("code",[s._v("sentence2 [SEP]")]),s._v(" 对应的 token type ID 都为 1")]),s._v(" "),t("p",[s._v("实际使用时，我们不需要去关注编码结果中是否包含 "),t("code",[s._v("token_type_ids")]),s._v(" 项，分词器会根据 checkpoint 自动调整输出格式")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-uncased"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsentence1_list "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"First sentence."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"This is the second sentence."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Third one."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\nsentence2_list "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"First sentence is short."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"The second sentence is very very very long."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"ok."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\ntokens "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    sentence1_list"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    sentence2_list"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    padding"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    truncation"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    return_tensors"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"pt"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'input_ids'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("{'input_ids': tensor([\n        [ 101, 2034, 6251, 1012,  102, 2034, 6251, 2003, 2460, 1012,  102,    0,\n            0,    0,    0,    0,    0,    0],\n        [ 101, 2023, 2003, 1996, 2117, 6251, 1012,  102, 1996, 2117, 6251, 2003,\n         2200, 2200, 2200, 2146, 1012,  102],\n        [ 101, 2353, 2028, 1012,  102, 7929, 1012,  102,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0]]), \n 'token_type_ids': tensor([\n        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), \n 'attention_mask': tensor([\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n}\ntorch.Size([3, 18])\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br")])]),t("p",[s._v("可以看到分词器成功地输出了形式为 "),t("code",[s._v("[CLS] sentence1 [SEP] sentence2 [SEP]")]),s._v(" 的 token 序列，并且三个序列都 padding 到了相同的长度")]),s._v(" "),t("h2",{attrs:{id:"添加-token"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#添加-token"}},[s._v("#")]),s._v(" 添加 Token")]),s._v(" "),t("p",[s._v("实际操作中，我们还经常会遇到输入中需要包含特殊标记符的情况，例如使用 "),t("code",[s._v("[ENT_START]")]),s._v(" 和 "),t("code",[s._v("[ENT_END]")]),s._v(" 标记出文本中的实体。由于这些自定义 token 并不在预训练模型原来的词表中，因此直接运用分词器处理就会出现问题。")]),s._v(" "),t("p",[s._v("例如直接使用 BERT 分词器处理下面的句子：")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-uncased"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsentence "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.'")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tokenize"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sentence"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("['two', '[', 'en', '#t', '_','start', ']', 'cars', '[', 'en', '#t', '_', 'end', ']', 'collided', 'i\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("由于分词器无法识别 "),t("code",[s._v("[ENT_START]")]),s._v(" 和 "),t("code",[s._v("[ENT_END]")]),s._v("，因此将它们都当作未知字符处理，例如 "),t("code",[s._v("[ENT_END]")]),s._v("被切分成了六个 token。")]),s._v(" "),t("p",[s._v("此外，一些领域的专业词汇，例如使用多个词语的缩写拼接而成的医学术语，同样也不在模型的词表中，因此也会出现上面的问题。此时我们就需要将这些新 token 添加到模型的词表中，让分词器与模型可以识别并处理这些 token。")]),s._v(" "),t("h3",{attrs:{id:"添加新-token"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#添加新-token"}},[s._v("#")]),s._v(" 添加新 token")]),s._v(" "),t("p",[s._v("Transformers 库提供了两种方式来添加新的 token，分别是")]),s._v(" "),t("ul",[t("li",[t("code",[s._v("add_tokens()")]),s._v(" 添加普通的 token：参数是新的 token 列表，如果 token 不在列表中，就会被添加到词表的最后")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("checkpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-uncased"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    \nnum_added_toks "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"new_token1"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"my_new-token2"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"We have added"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" num_added_toks"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"tokens"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("We have added 2 tokens\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("为了防止 token 已经包含在词表中，我们还可以预先对新 token 列表进行过滤")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("new_tokens "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"new_token1"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"my_new-token2"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\nnew_tokens "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("set")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("new_tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("set")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("vocab"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keys"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ntokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("list")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("new_tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("ul",[t("li",[t("code",[s._v("add_special_tokens()")]),s._v(" 添加特殊 token：参数是包含特殊 token 的字典，键值只能从 "),t("code",[s._v("bos_token")]),s._v(", "),t("code",[s._v("eos_token")]),s._v(", "),t("code",[s._v("unk_token")]),s._v(", "),t("code",[s._v("sep_token")]),s._v(", "),t("code",[s._v("pad_token")]),s._v(", "),t("code",[s._v("cls_token")]),s._v(", "),t("code",[s._v("mask_token")]),s._v(", "),t("code",[s._v("additional_special_tokens")]),s._v(" 中选择。同样地，如果 token 不在词表中，就会被添加到词表的最后。添加后，还可以通过特殊属性来访问这些 token，例如 "),t("code",[s._v("tokenizer.cls_token")]),s._v(" 就指向 "),t("code",[s._v("cls token")]),s._v("。")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("checkpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-uncased"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    \nspecial_tokens_dict "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"cls_token"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"[MY_CLS]"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n    \nnum_added_toks "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_special_tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("special_tokens_dict"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"We have added"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" num_added_toks"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"tokens"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    \n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("assert")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cls_token "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("==")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"[MY_CLS]"')]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("We have added 1 tokens\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("我们也可以使用 "),t("code",[s._v("add_tokens()")]),s._v(" 添加特殊的 token，只需要额外设置参数 "),t("code",[s._v("special_tokens=True")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("checkpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-uncased"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    \nnum_added_toks "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"[NEW_tok1]"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"[NEW_tok2]"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nnum_added_toks "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"[NEW_tok3]"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"[NEW_tok4]"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" special_tokens"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    \n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"We have added"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" num_added_toks"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"tokens"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tokenize"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'[NEW_tok1] Hello [NEW_tok2] [NEW_tok3] World [NEW_tok4]!'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("We have added 2 tokens\n['[new_tok1]', 'hello', '[new_tok2]', '[NEW_tok3]', 'world', '[NEW_tok4]', '!']\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br")])]),t("p",[s._v("对于前面的例子，很明显实体标记符 "),t("code",[s._v("[ENT_START]")]),s._v(" 和 "),t("code",[s._v("[ENT_END]")]),s._v(" 属于特殊的 token，因此按添加特殊 token 的方式进行")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-uncased"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nnum_added_toks "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'[ENT_START]'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'[ENT_END]'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" special_tokens"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# num_added_toks = tokenizer.add_special_tokens({'additional_special_tokens': ['[ENT_START]', '[ENT_END]']})")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"We have added"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" num_added_toks"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"tokens"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsentence "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.'")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tokenize"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sentence"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("We have added 2 tokens\n['two', '[ENT_START]', 'cars', '[ENT_END]', 'collided', 'in', 'a', '[ENT_START]', 'tunnel', '[ENT_END]', 'this', 'morning', '.']\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br")])]),t("p",[s._v("可以看到，分词器成功地将 "),t("code",[s._v("[ENT_START]")]),s._v(" 和 "),t("code",[s._v("[ENT_END]")]),s._v(" 识别为 token，并且保持大写")]),s._v(" "),t("h3",{attrs:{id:"调整-embedding-矩阵"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#调整-embedding-矩阵"}},[s._v("#")]),s._v(" 调整 embedding 矩阵")]),s._v(" "),t("p",[s._v("向词表中添加新 token 后，必须重置模型 embedding 矩阵的大小，也就是向矩阵中添加新 token 对应的 embedding，这样模型才能正常工作，讲 token 映射到对应的 embedding")]),s._v(" "),t("p",[s._v("调整 embedding 矩阵通过 "),t("code",[s._v("resize_token_embeddings()")]),s._v(" 函数实现，例如对于前面的例子")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" AutoModel\n\ncheckpoint "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-uncased"')]),s._v("\ntokenizer "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoModel"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'vocabulary size:'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("len")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nnum_added_toks "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_tokens"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'[ENT_START]'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'[ENT_END]'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" special_tokens"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"After we add"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" num_added_toks"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"tokens"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'vocabulary size:'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("len")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nmodel"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("resize_token_embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("len")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("word_embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("size"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Randomly generated matrix")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("word_embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("vocabulary size: 30522\nAfter we add 2 tokens\nvocabulary size: 30524\ntorch.Size([30524, 768])\n\ntensor([[-0.0325, -0.0224,  0.0044,  ..., -0.0088, -0.0078, -0.0110],\n        [-0.0005, -0.0167, -0.0009,  ...,  0.0110, -0.0282, -0.0013]],\n       grad_fn=<SliceBackward0>)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br")])]),t("p",[s._v("可以看到，在添加 "),t("code",[s._v("[ENT_START]")]),s._v(" 和 "),t("code",[s._v("[ENT_END]")]),s._v(" 之后，分词器的词表大小从 30522 增加到了 30524，模型 embedding 矩阵的大小也调整到了 "),t("eq",[t("span",{staticClass:"katex"},[t("span",{staticClass:"katex-mathml"},[t("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[t("semantics",[t("mrow",[t("mn",[s._v("30524")]),t("mo",[s._v("×")]),t("mn",[s._v("768")])],1),t("annotation",{attrs:{encoding:"application/x-tex"}},[s._v("30524\\times 768")])],1)],1)],1),t("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[t("span",{staticClass:"base"},[t("span",{staticClass:"strut",staticStyle:{height:"0.72777em","vertical-align":"-0.08333em"}}),t("span",{staticClass:"mord"},[s._v("3")]),t("span",{staticClass:"mord"},[s._v("0")]),t("span",{staticClass:"mord"},[s._v("5")]),t("span",{staticClass:"mord"},[s._v("2")]),t("span",{staticClass:"mord"},[s._v("4")]),t("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),t("span",{staticClass:"mbin"},[s._v("×")]),t("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),t("span",{staticClass:"base"},[t("span",{staticClass:"strut",staticStyle:{height:"0.64444em","vertical-align":"0em"}}),t("span",{staticClass:"mord"},[s._v("7")]),t("span",{staticClass:"mord"},[s._v("6")]),t("span",{staticClass:"mord"},[s._v("8")])])])])])],1),s._v(" "),t("p",[s._v("在默认情况下，新添加的 token 的embedding 是随机初始化的")]),s._v(" "),t("h2",{attrs:{id:"token-embedding-初始化"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#token-embedding-初始化"}},[s._v("#")]),s._v(" Token embedding 初始化")]),s._v(" "),t("p",[s._v("如果有充分的语料对模型进行微调或者继续预训练，那么将新添加 token 初始化为随机向量没什么问题。但是如果训练语料较少，甚至是只有很少语料的 few-shot learning 场景下，这种做法就存在问题。研究表明，在训练数据不够多的情况下，这些新添加 token 的 embedding 只会在初始值附近小幅波动。换句话说，即使经过训练，它们的值事实上还是随机的。")]),s._v(" "),t("h3",{attrs:{id:"直接赋值"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#直接赋值"}},[s._v("#")]),s._v(" 直接赋值")]),s._v(" "),t("p",[s._v("因此，在很多情况下，我们需要手工初始化新添加 token 的embeding")]),s._v(" "),t("p",[s._v("例如，我们将上面例子中两个新 token 的embedding 都初始化为全 0 向量")]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" torch\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("no_grad"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("word_embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("zeros"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("config"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("hidden_size"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" requires_grad"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("word_embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SliceBackward0>)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br")])]),t("p",[s._v("初始化 embedding 的过程并不可导，因此这里通过 "),t("code",[s._v("torch.no_grad()")]),s._v(" 来暂停梯度的计算")]),s._v(" "),t("p",[s._v("在现实场景中，更为常见的做法是使用已有 token 的 embedding 来初始化新 token，例如，对于上面的例子，我们可以将 "),t("code",[s._v("[ENT_START]")]),s._v(" 和 "),t("code",[s._v("[ENT_END]")]),s._v(' 的值都初始化为 "entity" token 对应的 embedding')]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" torch\n\ntoken_id "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("convert_tokens_to_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'entity'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ntoken_embedding "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("word_embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("token_id"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("token_id"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("no_grad"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" i "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" num_added_toks"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("word_embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v("i"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" token_embedding"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("clone"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("detach"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("requires_grad_"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("word_embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("9178\ntensor([[-0.0039, -0.0131, -0.0946,  ..., -0.0223,  0.0107, -0.0419],\n        [-0.0039, -0.0131, -0.0946,  ..., -0.0223,  0.0107, -0.0419]],\n       grad_fn=<SliceBackward0>)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("p",[s._v("因为 token ID 就是 token 在 embedding 矩阵中的索引，因此这里我们直接通过 "),t("code",[s._v("weight[token_id]")]),s._v(" 取出“entity”对应的 embedding。")]),s._v(" "),t("p",[s._v("可以看到最终结果符合我们的预期，"),t("code",[s._v("[ENT_START]")]),s._v(" 和 "),t("code",[s._v("[ENT_END]")]),s._v(" 都被初始化为相同的 embedding")]),s._v(" "),t("h3",{attrs:{id:"初始化已有-token-的值"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#初始化已有-token-的值"}},[s._v("#")]),s._v(" 初始化已有 token 的值")]),s._v(" "),t("p",[s._v("更为高级的做法是根据新添加 token 的语义来进行初始化。例如将值初始化为 token 语义描述中所有 token 的平均值，假设新 token "),t("eq",[t("span",{staticClass:"katex"},[t("span",{staticClass:"katex-mathml"},[t("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[t("semantics",[t("mrow",[t("msub",[t("mi",[s._v("t")]),t("mi",[s._v("i")])],1)],1),t("annotation",{attrs:{encoding:"application/x-tex"}},[s._v("t_i")])],1)],1)],1),t("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[t("span",{staticClass:"base"},[t("span",{staticClass:"strut",staticStyle:{height:"0.76508em","vertical-align":"-0.15em"}}),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord mathdefault"},[s._v("t")]),t("span",{staticClass:"msupsub"},[t("span",{staticClass:"vlist-t vlist-t2"},[t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.31166399999999994em"}},[t("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),t("span",{staticClass:"sizing reset-size6 size3 mtight"},[t("span",{staticClass:"mord mathdefault mtight"},[s._v("i")])])])]),t("span",{staticClass:"vlist-s"},[s._v("​")])]),t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[t("span")])])])])])])])])]),s._v("的语义描述为"),t("eq",[t("span",{staticClass:"katex"},[t("span",{staticClass:"katex-mathml"},[t("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[t("semantics",[t("mrow",[t("msub",[t("mi",[s._v("w")]),t("mrow",[t("mi",[s._v("i")]),t("mo",{attrs:{separator:"true"}},[s._v(",")]),t("mn",[s._v("1")])],1)],1),t("mo",{attrs:{separator:"true"}},[s._v(",")]),t("msub",[t("mi",[s._v("w")]),t("mrow",[t("mi",[s._v("i")]),t("mo",{attrs:{separator:"true"}},[s._v(",")]),t("mn",[s._v("2")])],1)],1),t("mo",{attrs:{separator:"true"}},[s._v(",")]),t("mo",[s._v("…")]),t("mo",{attrs:{separator:"true"}},[s._v(",")]),t("msub",[t("mi",[s._v("w")]),t("mrow",[t("mi",[s._v("i")]),t("mo",{attrs:{separator:"true"}},[s._v(",")]),t("mi",[s._v("n")])],1)],1)],1),t("annotation",{attrs:{encoding:"application/x-tex"}},[s._v("w_{i,1}, w_{i,2}, \\ldots, w_{i,n}")])],1)],1)],1),t("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[t("span",{staticClass:"base"},[t("span",{staticClass:"strut",staticStyle:{height:"0.716668em","vertical-align":"-0.286108em"}}),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.02691em"}},[s._v("w")]),t("span",{staticClass:"msupsub"},[t("span",{staticClass:"vlist-t vlist-t2"},[t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.311664em"}},[t("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"-0.02691em","margin-right":"0.05em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),t("span",{staticClass:"sizing reset-size6 size3 mtight"},[t("span",{staticClass:"mord mtight"},[t("span",{staticClass:"mord mathdefault mtight"},[s._v("i")]),t("span",{staticClass:"mpunct mtight"},[s._v(",")]),t("span",{staticClass:"mord mtight"},[s._v("1")])])])])]),t("span",{staticClass:"vlist-s"},[s._v("​")])]),t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.286108em"}},[t("span")])])])])]),t("span",{staticClass:"mpunct"},[s._v(",")]),t("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.02691em"}},[s._v("w")]),t("span",{staticClass:"msupsub"},[t("span",{staticClass:"vlist-t vlist-t2"},[t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.311664em"}},[t("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"-0.02691em","margin-right":"0.05em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),t("span",{staticClass:"sizing reset-size6 size3 mtight"},[t("span",{staticClass:"mord mtight"},[t("span",{staticClass:"mord mathdefault mtight"},[s._v("i")]),t("span",{staticClass:"mpunct mtight"},[s._v(",")]),t("span",{staticClass:"mord mtight"},[s._v("2")])])])])]),t("span",{staticClass:"vlist-s"},[s._v("​")])]),t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.286108em"}},[t("span")])])])])]),t("span",{staticClass:"mpunct"},[s._v(",")]),t("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),t("span",{staticClass:"minner"},[s._v("…")]),t("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),t("span",{staticClass:"mpunct"},[s._v(",")]),t("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.02691em"}},[s._v("w")]),t("span",{staticClass:"msupsub"},[t("span",{staticClass:"vlist-t vlist-t2"},[t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.311664em"}},[t("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"-0.02691em","margin-right":"0.05em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),t("span",{staticClass:"sizing reset-size6 size3 mtight"},[t("span",{staticClass:"mord mtight"},[t("span",{staticClass:"mord mathdefault mtight"},[s._v("i")]),t("span",{staticClass:"mpunct mtight"},[s._v(",")]),t("span",{staticClass:"mord mathdefault mtight"},[s._v("n")])])])])]),t("span",{staticClass:"vlist-s"},[s._v("​")])]),t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.286108em"}},[t("span")])])])])])])])])]),s._v("，那么初始化 "),t("eq",[t("span",{staticClass:"katex"},[t("span",{staticClass:"katex-mathml"},[t("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[t("semantics",[t("mrow",[t("msub",[t("mi",[s._v("t")]),t("mi",[s._v("i")])],1)],1),t("annotation",{attrs:{encoding:"application/x-tex"}},[s._v("t_i")])],1)],1)],1),t("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[t("span",{staticClass:"base"},[t("span",{staticClass:"strut",staticStyle:{height:"0.76508em","vertical-align":"-0.15em"}}),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord mathdefault"},[s._v("t")]),t("span",{staticClass:"msupsub"},[t("span",{staticClass:"vlist-t vlist-t2"},[t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.31166399999999994em"}},[t("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),t("span",{staticClass:"sizing reset-size6 size3 mtight"},[t("span",{staticClass:"mord mathdefault mtight"},[s._v("i")])])])]),t("span",{staticClass:"vlist-s"},[s._v("​")])]),t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[t("span")])])])])])])])])]),s._v("的 embedding 为：")],1),s._v(" "),t("section",[t("eqn",[t("span",{staticClass:"katex-display"},[t("span",{staticClass:"katex"},[t("span",{staticClass:"katex-mathml"},[t("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[t("semantics",[t("mrow",[t("mi",{attrs:{mathvariant:"bold-italic"}},[s._v("E")]),t("mo",{attrs:{stretchy:"false"}},[s._v("(")]),t("msub",[t("mi",[s._v("t")]),t("mi",[s._v("i")])],1),t("mo",{attrs:{stretchy:"false"}},[s._v(")")]),t("mo",[s._v("=")]),t("mfrac",[t("mn",[s._v("1")]),t("mi",[s._v("n")])],1),t("munderover",[t("mo",[s._v("∑")]),t("mrow",[t("mi",[s._v("j")]),t("mo",[s._v("=")]),t("mn",[s._v("1")])],1),t("mi",[s._v("n")])],1),t("mi",{attrs:{mathvariant:"bold-italic"}},[s._v("E")]),t("mo",{attrs:{stretchy:"false"}},[s._v("(")]),t("msub",[t("mi",[s._v("w")]),t("mrow",[t("mi",[s._v("i")]),t("mo",{attrs:{separator:"true"}},[s._v(",")]),t("mi",[s._v("j")])],1)],1),t("mo",{attrs:{stretchy:"false"}},[s._v(")")])],1),t("annotation",{attrs:{encoding:"application/x-tex"}},[s._v("\n\\boldsymbol{E}(t_i) = \\frac{1}{n} \\sum_{j=1}^{n} \\boldsymbol{E}(w_{i,j})\n")])],1)],1)],1),t("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[t("span",{staticClass:"base"},[t("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord"},[t("span",{staticClass:"mord boldsymbol",staticStyle:{"margin-right":"0.05451em"}},[s._v("E")])])]),t("span",{staticClass:"mopen"},[s._v("(")]),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord mathdefault"},[s._v("t")]),t("span",{staticClass:"msupsub"},[t("span",{staticClass:"vlist-t vlist-t2"},[t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.31166399999999994em"}},[t("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),t("span",{staticClass:"sizing reset-size6 size3 mtight"},[t("span",{staticClass:"mord mathdefault mtight"},[s._v("i")])])])]),t("span",{staticClass:"vlist-s"},[s._v("​")])]),t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[t("span")])])])])]),t("span",{staticClass:"mclose"},[s._v(")")]),t("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),t("span",{staticClass:"mrel"},[s._v("=")]),t("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),t("span",{staticClass:"base"},[t("span",{staticClass:"strut",staticStyle:{height:"3.0651740000000007em","vertical-align":"-1.4137769999999998em"}}),t("span",{staticClass:"mord"},[t("span",{staticClass:"mopen nulldelimiter"}),t("span",{staticClass:"mfrac"},[t("span",{staticClass:"vlist-t vlist-t2"},[t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"1.32144em"}},[t("span",{staticStyle:{top:"-2.314em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord mathdefault"},[s._v("n")])])]),t("span",{staticStyle:{top:"-3.23em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),t("span",{staticClass:"frac-line",staticStyle:{"border-bottom-width":"0.04em"}})]),t("span",{staticStyle:{top:"-3.677em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord"},[s._v("1")])])])]),t("span",{staticClass:"vlist-s"},[s._v("​")])]),t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.686em"}},[t("span")])])])]),t("span",{staticClass:"mclose nulldelimiter"})]),t("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),t("span",{staticClass:"mop op-limits"},[t("span",{staticClass:"vlist-t vlist-t2"},[t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"1.6513970000000007em"}},[t("span",{staticStyle:{top:"-1.872331em","margin-left":"0em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"3.05em"}}),t("span",{staticClass:"sizing reset-size6 size3 mtight"},[t("span",{staticClass:"mord mtight"},[t("span",{staticClass:"mord mathdefault mtight",staticStyle:{"margin-right":"0.05724em"}},[s._v("j")]),t("span",{staticClass:"mrel mtight"},[s._v("=")]),t("span",{staticClass:"mord mtight"},[s._v("1")])])])]),t("span",{staticStyle:{top:"-3.050005em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"3.05em"}}),t("span",[t("span",{staticClass:"mop op-symbol large-op"},[s._v("∑")])])]),t("span",{staticStyle:{top:"-4.3000050000000005em","margin-left":"0em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"3.05em"}}),t("span",{staticClass:"sizing reset-size6 size3 mtight"},[t("span",{staticClass:"mord mtight"},[t("span",{staticClass:"mord mathdefault mtight"},[s._v("n")])])])])]),t("span",{staticClass:"vlist-s"},[s._v("​")])]),t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"1.4137769999999998em"}},[t("span")])])])]),t("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord"},[t("span",{staticClass:"mord boldsymbol",staticStyle:{"margin-right":"0.05451em"}},[s._v("E")])])]),t("span",{staticClass:"mopen"},[s._v("(")]),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.02691em"}},[s._v("w")]),t("span",{staticClass:"msupsub"},[t("span",{staticClass:"vlist-t vlist-t2"},[t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.311664em"}},[t("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"-0.02691em","margin-right":"0.05em"}},[t("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),t("span",{staticClass:"sizing reset-size6 size3 mtight"},[t("span",{staticClass:"mord mtight"},[t("span",{staticClass:"mord mathdefault mtight"},[s._v("i")]),t("span",{staticClass:"mpunct mtight"},[s._v(",")]),t("span",{staticClass:"mord mathdefault mtight",staticStyle:{"margin-right":"0.05724em"}},[s._v("j")])])])])]),t("span",{staticClass:"vlist-s"},[s._v("​")])]),t("span",{staticClass:"vlist-r"},[t("span",{staticClass:"vlist",staticStyle:{height:"0.286108em"}},[t("span")])])])])]),t("span",{staticClass:"mclose"},[s._v(")")])])])])])])],1),t("p",[s._v("这里"),t("eq",[t("span",{staticClass:"katex"},[t("span",{staticClass:"katex-mathml"},[t("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[t("semantics",[t("mrow",[t("mi",{attrs:{mathvariant:"bold-italic"}},[s._v("E")])],1),t("annotation",{attrs:{encoding:"application/x-tex"}},[s._v("\\boldsymbol{E}")])],1)],1)],1),t("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[t("span",{staticClass:"base"},[t("span",{staticClass:"strut",staticStyle:{height:"0.68611em","vertical-align":"0em"}}),t("span",{staticClass:"mord"},[t("span",{staticClass:"mord"},[t("span",{staticClass:"mord boldsymbol",staticStyle:{"margin-right":"0.05451em"}},[s._v("E")])])])])])])]),s._v(" 表示预训练模型的 embedding 矩阵。对于上面的例子，我们可以分别为 "),t("code",[s._v("[ENT_START]")]),s._v(" 和 "),t("code",[s._v("[ENT_END]")]),s._v(" 编写对应的描述，然后再对它们的值进行初始化：")],1),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("descriptions "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'start of entity'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'end of entity'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("no_grad"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" i"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" token "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("enumerate")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("reversed")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("descriptions"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" start"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        tokenized "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tokenize"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("token"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenized"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        tokenized_ids "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("convert_tokens_to_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenized"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        new_embedding "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("word_embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("tokenized_ids"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("mean"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("axis"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("word_embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v("i"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" new_embedding"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("clone"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("detach"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("requires_grad_"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("word_embeddings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("['end', 'of', 'entity']\n['start', 'of', 'entity']\ntensor([[-0.0340, -0.0144, -0.0441,  ..., -0.0016,  0.0318, -0.0151],\n        [-0.0060, -0.0202, -0.0312,  ..., -0.0084,  0.0193, -0.0296]],\n       grad_fn=<SliceBackward0>)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br")])]),t("p",[s._v("可以看到，这里成功地将 "),t("code",[s._v("[ENT_START]")]),s._v(" 的 embedding 初始化为“start”、“of”、“entity”三个 token 的平均值，将 "),t("code",[s._v("[ENT_END]")]),s._v(" 初始化为“end”、“of”、“entity”的平均值。")])])}),[],!1,null,null,null);t.default=e.exports}}]);