<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>模型与分词器 | Martian148&#39;s blog</title>
    <meta name="generator" content="VuePress 1.9.9">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
    <link rel="icon" href="/img/favicon.ico">
    <meta name="description" content="Martian148的空间">
    <meta name="keywords" content="martian的小站">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/assets/css/0.styles.54ae510b.css" as="style"><link rel="preload" href="/assets/js/app.627fef7e.js" as="script"><link rel="preload" href="/assets/js/2.30da42f9.js" as="script"><link rel="preload" href="/assets/js/25.6fcc0217.js" as="script"><link rel="prefetch" href="/assets/js/10.ec97f902.js"><link rel="prefetch" href="/assets/js/100.63f4c3df.js"><link rel="prefetch" href="/assets/js/101.df80084e.js"><link rel="prefetch" href="/assets/js/102.7fe84d94.js"><link rel="prefetch" href="/assets/js/103.afbd6a25.js"><link rel="prefetch" href="/assets/js/104.01c82dc6.js"><link rel="prefetch" href="/assets/js/105.d0055d9c.js"><link rel="prefetch" href="/assets/js/106.64e69f3e.js"><link rel="prefetch" href="/assets/js/107.221ef5b2.js"><link rel="prefetch" href="/assets/js/108.8743af14.js"><link rel="prefetch" href="/assets/js/109.028a0087.js"><link rel="prefetch" href="/assets/js/11.652d8466.js"><link rel="prefetch" href="/assets/js/110.38145915.js"><link rel="prefetch" href="/assets/js/111.12cd5037.js"><link rel="prefetch" href="/assets/js/112.fd157c31.js"><link rel="prefetch" href="/assets/js/113.ec346a58.js"><link rel="prefetch" href="/assets/js/114.b9c3f067.js"><link rel="prefetch" href="/assets/js/115.7b6db8ab.js"><link rel="prefetch" href="/assets/js/116.18034a67.js"><link rel="prefetch" href="/assets/js/117.ec67c7c0.js"><link rel="prefetch" href="/assets/js/118.fa6de5a9.js"><link rel="prefetch" href="/assets/js/119.6f1462ad.js"><link rel="prefetch" href="/assets/js/12.d40e88a5.js"><link rel="prefetch" href="/assets/js/120.6ef1a324.js"><link rel="prefetch" href="/assets/js/121.9119166a.js"><link rel="prefetch" href="/assets/js/122.ffc19b76.js"><link rel="prefetch" href="/assets/js/123.281196f9.js"><link rel="prefetch" href="/assets/js/124.549c25db.js"><link rel="prefetch" href="/assets/js/125.44d21b24.js"><link rel="prefetch" href="/assets/js/126.c57c4c30.js"><link rel="prefetch" href="/assets/js/127.29c04e07.js"><link rel="prefetch" href="/assets/js/128.a04a0a1f.js"><link rel="prefetch" href="/assets/js/129.7922193d.js"><link rel="prefetch" href="/assets/js/13.30b29a98.js"><link rel="prefetch" href="/assets/js/130.2c778256.js"><link rel="prefetch" href="/assets/js/131.0700d1da.js"><link rel="prefetch" href="/assets/js/132.8cab1748.js"><link rel="prefetch" href="/assets/js/133.8129212d.js"><link rel="prefetch" href="/assets/js/134.670af809.js"><link rel="prefetch" href="/assets/js/135.1480dd61.js"><link rel="prefetch" href="/assets/js/136.7d63e743.js"><link rel="prefetch" href="/assets/js/137.32ebf5cf.js"><link rel="prefetch" href="/assets/js/138.691bad0a.js"><link rel="prefetch" href="/assets/js/139.8f2eded5.js"><link rel="prefetch" href="/assets/js/14.360faf22.js"><link rel="prefetch" href="/assets/js/140.e46c0701.js"><link rel="prefetch" href="/assets/js/141.d22eb28b.js"><link rel="prefetch" href="/assets/js/142.20e572fd.js"><link rel="prefetch" href="/assets/js/143.a0cabcf1.js"><link rel="prefetch" href="/assets/js/144.e8580262.js"><link rel="prefetch" href="/assets/js/145.dabeec6f.js"><link rel="prefetch" href="/assets/js/146.a0b4ee55.js"><link rel="prefetch" href="/assets/js/147.b9106409.js"><link rel="prefetch" href="/assets/js/148.86ba4ea7.js"><link rel="prefetch" href="/assets/js/149.d3d5e930.js"><link rel="prefetch" href="/assets/js/15.c0932b47.js"><link rel="prefetch" href="/assets/js/150.bb79fb43.js"><link rel="prefetch" href="/assets/js/151.88d24ce9.js"><link rel="prefetch" href="/assets/js/152.db0f9356.js"><link rel="prefetch" href="/assets/js/16.c59a48aa.js"><link rel="prefetch" href="/assets/js/17.57dd702d.js"><link rel="prefetch" href="/assets/js/18.c0ffa5d8.js"><link rel="prefetch" href="/assets/js/19.57dee162.js"><link rel="prefetch" href="/assets/js/20.15bc0ddd.js"><link rel="prefetch" href="/assets/js/21.d9d1568b.js"><link rel="prefetch" href="/assets/js/22.cbff67f7.js"><link rel="prefetch" href="/assets/js/23.fdf16c27.js"><link rel="prefetch" href="/assets/js/24.689b3fcb.js"><link rel="prefetch" href="/assets/js/26.7abe98c5.js"><link rel="prefetch" href="/assets/js/27.747380a5.js"><link rel="prefetch" href="/assets/js/28.7dffece5.js"><link rel="prefetch" href="/assets/js/29.b9c6bd73.js"><link rel="prefetch" href="/assets/js/3.0e4ffe6c.js"><link rel="prefetch" href="/assets/js/30.ec4dce10.js"><link rel="prefetch" href="/assets/js/31.d777ebcb.js"><link rel="prefetch" href="/assets/js/32.57d6dcbd.js"><link rel="prefetch" href="/assets/js/33.99fc201b.js"><link rel="prefetch" href="/assets/js/34.e12fb274.js"><link rel="prefetch" href="/assets/js/35.fe5e856c.js"><link rel="prefetch" href="/assets/js/36.73a02d16.js"><link rel="prefetch" href="/assets/js/37.d3cd5fe8.js"><link rel="prefetch" href="/assets/js/38.a002a285.js"><link rel="prefetch" href="/assets/js/39.34dbc413.js"><link rel="prefetch" href="/assets/js/4.1acdcc39.js"><link rel="prefetch" href="/assets/js/40.4352b4ea.js"><link rel="prefetch" href="/assets/js/41.362bfe87.js"><link rel="prefetch" href="/assets/js/42.ac189401.js"><link rel="prefetch" href="/assets/js/43.dbb7856a.js"><link rel="prefetch" href="/assets/js/44.05cc402a.js"><link rel="prefetch" href="/assets/js/45.6b3de4d0.js"><link rel="prefetch" href="/assets/js/46.bb8b375d.js"><link rel="prefetch" href="/assets/js/47.cdedba5f.js"><link rel="prefetch" href="/assets/js/48.0576602d.js"><link rel="prefetch" href="/assets/js/49.181dd329.js"><link rel="prefetch" href="/assets/js/5.5f98dcdd.js"><link rel="prefetch" href="/assets/js/50.e5be72c1.js"><link rel="prefetch" href="/assets/js/51.b3d21005.js"><link rel="prefetch" href="/assets/js/52.fa2ecb3c.js"><link rel="prefetch" href="/assets/js/53.5233f62a.js"><link rel="prefetch" href="/assets/js/54.b952b4e5.js"><link rel="prefetch" href="/assets/js/55.807e1842.js"><link rel="prefetch" href="/assets/js/56.4a657413.js"><link rel="prefetch" href="/assets/js/57.449613aa.js"><link rel="prefetch" href="/assets/js/58.ca9da6b5.js"><link rel="prefetch" href="/assets/js/59.50fd3b47.js"><link rel="prefetch" href="/assets/js/6.488d725c.js"><link rel="prefetch" href="/assets/js/60.b7c826ad.js"><link rel="prefetch" href="/assets/js/61.d55318b1.js"><link rel="prefetch" href="/assets/js/62.072fd5d1.js"><link rel="prefetch" href="/assets/js/63.569c5877.js"><link rel="prefetch" href="/assets/js/64.09fcf62c.js"><link rel="prefetch" href="/assets/js/65.e7dfd454.js"><link rel="prefetch" href="/assets/js/66.c0e8e4ac.js"><link rel="prefetch" href="/assets/js/67.4e598f07.js"><link rel="prefetch" href="/assets/js/68.95c362b9.js"><link rel="prefetch" href="/assets/js/69.ac7147f5.js"><link rel="prefetch" href="/assets/js/7.7e577db5.js"><link rel="prefetch" href="/assets/js/70.5fe5cb16.js"><link rel="prefetch" href="/assets/js/71.691cf5b2.js"><link rel="prefetch" href="/assets/js/72.07110900.js"><link rel="prefetch" href="/assets/js/73.c765a680.js"><link rel="prefetch" href="/assets/js/74.1a3b4a8b.js"><link rel="prefetch" href="/assets/js/75.798728d7.js"><link rel="prefetch" href="/assets/js/76.14f437ed.js"><link rel="prefetch" href="/assets/js/77.911fb543.js"><link rel="prefetch" href="/assets/js/78.58826a21.js"><link rel="prefetch" href="/assets/js/79.595a36dd.js"><link rel="prefetch" href="/assets/js/8.8e1f4b0e.js"><link rel="prefetch" href="/assets/js/80.7701d93d.js"><link rel="prefetch" href="/assets/js/81.5e21cbe7.js"><link rel="prefetch" href="/assets/js/82.c033980f.js"><link rel="prefetch" href="/assets/js/83.abdcb819.js"><link rel="prefetch" href="/assets/js/84.1001029e.js"><link rel="prefetch" href="/assets/js/85.2cfab87f.js"><link rel="prefetch" href="/assets/js/86.1e82b239.js"><link rel="prefetch" href="/assets/js/87.dce4c180.js"><link rel="prefetch" href="/assets/js/88.5dfd512d.js"><link rel="prefetch" href="/assets/js/89.2430a8c4.js"><link rel="prefetch" href="/assets/js/9.5d6b5ce0.js"><link rel="prefetch" href="/assets/js/90.6c58ef61.js"><link rel="prefetch" href="/assets/js/91.32dc84dc.js"><link rel="prefetch" href="/assets/js/92.89d5027c.js"><link rel="prefetch" href="/assets/js/93.dba19ca2.js"><link rel="prefetch" href="/assets/js/94.08f22d02.js"><link rel="prefetch" href="/assets/js/95.a505acdc.js"><link rel="prefetch" href="/assets/js/96.7dfa71ac.js"><link rel="prefetch" href="/assets/js/97.49d4f37d.js"><link rel="prefetch" href="/assets/js/98.da6886fa.js"><link rel="prefetch" href="/assets/js/99.3a892131.js">
    <link rel="stylesheet" href="/assets/css/0.styles.54ae510b.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/logo.png" alt="Martian148's blog" class="logo"> <span class="site-name can-hide">Martian148's blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="计算机科学" class="dropdown-title"><a href="/cs/" class="link-title">计算机科学</a> <span class="title" style="display:none;">计算机科学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/cs/ICPC-notes/" class="nav-link">ICPC 算法笔记</a></li><li class="dropdown-item"><!----> <a href="/cs/ICPC-solution/" class="nav-link">ICPC 算法题解</a></li><li class="dropdown-item"><!----> <a href="/pages/eeed19/" class="nav-link">体系结构</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数学" class="dropdown-title"><a href="/math/" class="link-title">数学</a> <span class="title" style="display:none;">数学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/7e8888/" class="nav-link">高等数学</a></li><li class="dropdown-item"><!----> <a href="/pages/a7bf6e/" class="nav-link">线性代数</a></li><li class="dropdown-item"><!----> <a href="/pages/77f439/" class="nav-link">概率论与数理统计</a></li><li class="dropdown-item"><!----> <a href="/pages/cb1830/" class="nav-link">具体数学</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="更多" class="dropdown-title"><a href="/more/" class="link-title">更多</a> <span class="title" style="display:none;">更多</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/91d78e/" class="nav-link">Martian148的奇思妙想</a></li><li class="dropdown-item"><!----> <a href="/pages/964166/" class="nav-link">游记</a></li><li class="dropdown-item"><!----> <a href="/pages/9768eb/" class="nav-link">通识课笔记</a></li></ul></div></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="收藏" class="dropdown-title"><a href="/pages/beb6c0bd8a66cea6/" class="link-title">收藏</a> <span class="title" style="display:none;">收藏</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/beb6c0bd8a66cea6/" class="nav-link">useful 网站</a></li><li class="dropdown-item"><!----> <a href="/friends/" class="nav-link">友情链接</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="https://pic-1301573324.cos.ap-chengdu.myqcloud.com/20240731173832.png"> <div class="blogger-info"><h3>Martian148</h3> <span>一只热爱文科的理科生</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="计算机科学" class="dropdown-title"><a href="/cs/" class="link-title">计算机科学</a> <span class="title" style="display:none;">计算机科学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/cs/ICPC-notes/" class="nav-link">ICPC 算法笔记</a></li><li class="dropdown-item"><!----> <a href="/cs/ICPC-solution/" class="nav-link">ICPC 算法题解</a></li><li class="dropdown-item"><!----> <a href="/pages/eeed19/" class="nav-link">体系结构</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数学" class="dropdown-title"><a href="/math/" class="link-title">数学</a> <span class="title" style="display:none;">数学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/7e8888/" class="nav-link">高等数学</a></li><li class="dropdown-item"><!----> <a href="/pages/a7bf6e/" class="nav-link">线性代数</a></li><li class="dropdown-item"><!----> <a href="/pages/77f439/" class="nav-link">概率论与数理统计</a></li><li class="dropdown-item"><!----> <a href="/pages/cb1830/" class="nav-link">具体数学</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="更多" class="dropdown-title"><a href="/more/" class="link-title">更多</a> <span class="title" style="display:none;">更多</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/91d78e/" class="nav-link">Martian148的奇思妙想</a></li><li class="dropdown-item"><!----> <a href="/pages/964166/" class="nav-link">游记</a></li><li class="dropdown-item"><!----> <a href="/pages/9768eb/" class="nav-link">通识课笔记</a></li></ul></div></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="收藏" class="dropdown-title"><a href="/pages/beb6c0bd8a66cea6/" class="link-title">收藏</a> <span class="title" style="display:none;">收藏</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/beb6c0bd8a66cea6/" class="nav-link">useful 网站</a></li><li class="dropdown-item"><!----> <a href="/friends/" class="nav-link">友情链接</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>ACM - ICPC</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>编程语言</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>体系结构</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Web</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>人工智能</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/2447c3/" class="sidebar-link">机器学习笔记</a></li><li><a href="/pages/ceec75/" class="sidebar-link">《python科学计算入门》学习笔记</a></li><li><a href="/pages/99ffeb/" class="sidebar-link">LLM101 NLP学习笔记</a></li><li><a href="/pages/e79ad1/" class="sidebar-link">打破信息差，跟着水导学写论文</a></li><li><a href="/pages/a46255/" class="sidebar-link">一个简单的 PyTorch 图像分类器</a></li><li><a href="/pages/12a5b3/" aria-current="page" class="active sidebar-link">模型与分词器</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/pages/12a5b3/#模型" class="sidebar-link">模型</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#加载模型" class="sidebar-link">加载模型</a></li><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#保存模型" class="sidebar-link">保存模型</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/12a5b3/#分词器" class="sidebar-link">分词器</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#分词策略" class="sidebar-link">分词策略</a></li><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#加载与保存分词器" class="sidebar-link">加载与保存分词器</a></li><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#编码与解码文本" class="sidebar-link">编码与解码文本</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/12a5b3/#处理多段文本" class="sidebar-link">处理多段文本</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#padding-操作" class="sidebar-link">Padding 操作</a></li><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#attention-mask" class="sidebar-link">Attention Mask</a></li><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#直接使用分词器" class="sidebar-link">直接使用分词器</a></li><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#编码句子对" class="sidebar-link">编码句子对</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/12a5b3/#添加-token" class="sidebar-link">添加 Token</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#添加新-token" class="sidebar-link">添加新 token</a></li><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#调整-embedding-矩阵" class="sidebar-link">调整 embedding 矩阵</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/12a5b3/#token-embedding-初始化" class="sidebar-link">Token embedding 初始化</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#直接赋值" class="sidebar-link">直接赋值</a></li><li class="sidebar-sub-header level3"><a href="/pages/12a5b3/#初始化已有-token-的值" class="sidebar-link">初始化已有 token 的值</a></li></ul></li></ul></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>计算机网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>数据库</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>编程工具</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/cs/#计算机科学" data-v-06225672>计算机科学</a></li><li data-v-06225672><a href="/cs/#人工智能" data-v-06225672>人工智能</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/chengyiwei" target="_blank" title="作者" class="beLink" data-v-06225672>martian148</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-09-17</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">模型与分词器<!----></h1> <!----> <div class="theme-vdoing-content content__default"><h1 id="模型与分词器"><a href="#模型与分词器" class="header-anchor">#</a> 模型与分词器</h1> <h2 id="模型"><a href="#模型" class="header-anchor">#</a> 模型</h2> <p>可以直接使用对应的 <code>Model</code> 类，或者使用 <code>AutoModel</code> 根据 checkpoint 自动加载模型</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertModel

model <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;bert-base-cased&quot;</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>**在大部分情况下，我们都应该使用 <code>AutoModel</code> 来加载模型。**这样如果我们想要使用另一个模型（比如把 BERT 换成 RoBERTa），只需修改 checkpoint，其他代码可以保持不变。</p> <h3 id="加载模型"><a href="#加载模型" class="header-anchor">#</a> 加载模型</h3> <p>所有储存在 HuggingFace Model Hub 上的模型 都可以通过 <code>Modelfrom_pretrained()</code> 来加载权重</p> <p>参数可以是和上面使用名字来加载模型，也可以使用本地路径来加载模型</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertModel

model <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;./models/bert/&quot;</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><code>Model.from_pretrained()</code> 会自动缓存下载的模型权重，默认保存到<code>~/.cache/huggingface/transformers</code>，我们也可以通过 HF_HOME 环境变量自定义缓存目录。</p> <p>部分模型的 Hub 页面会包含很多文件，我们通常只需要下载模型对应的 <code>config.json</code> 和 <code>pytorch_model.bin</code>，以及分词器对应的 <code>tokenizer.json</code>、<code>tokenizer_config.json</code> 和 <code>vocab.txt</code>。</p> <h3 id="保存模型"><a href="#保存模型" class="header-anchor">#</a> 保存模型</h3> <p>保存模型通过调用 Model.save_pretrained() 函数实现，例如保存加载的 BERT 模型：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModel

model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;bert-base-cased&quot;</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">&quot;./models/bert-base-cased/&quot;</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这会在保存路径下创建两个文件：</p> <ul><li><code>config.json</code>：模型配置文件，存储模型结构参数，例如 Transformer 层数、特征空间维度等；</li> <li><code>pytorch_model.bin</code>：又称为 state dictionary，存储模型的权重。</li></ul> <p>简单来说，配置文件记录模型的结构，模型权重记录模型的参数，这两个文件缺一不可。我们自己保存的模型同样通过 <code>Model.from_pretrained()</code> 加载，只需要传递保存目录的路径。</p> <h2 id="分词器"><a href="#分词器" class="header-anchor">#</a> 分词器</h2> <p>由于神经网络模型不能直接处理文本，因此，我们要先把文本转成数字，这个过程被称为 Encoding，包含两个步骤：</p> <ol><li>使用分词器 tokenizer 将文本按词、子词，字符切分为 tokens</li> <li>将所有的 token 映射到对应的 token ID</li></ol> <h3 id="分词策略"><a href="#分词策略" class="header-anchor">#</a> 分词策略</h3> <p>分词策略可以分为以下几种：</p> <ul><li><strong>按照词切分（Word-based）</strong></li></ul> <p><img src="https://pic-1301573324.cos.ap-chengdu.myqcloud.com/20250913103649.png" alt="image.png"></p> <p>例如，直接利用 Python 的 <code>split()</code> 函数按照空格进行分词</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>tokenized_text <span class="token operator">=</span> <span class="token string">&quot;Jim Henson was a puppeteer&quot;</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokenized_text<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>['Jim', 'Henson', 'was', 'a', 'puppeteer']
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这种策略的问题是会将文本中所有出现过的独立片段都作为不同的 token，从而产生巨大的词表，例如 “dog” 和 “dogs”，如果给它们赋予不同的编号就无法表示出这种关联性</p> <p>当遇到不在词表中的词时，分词器会使用一个专门的 <code>[UNK]</code> token 来表示它是 unknown 的。显然，如果分词结果中包含很多 <code>[UNK]</code> 就意味着丢失了很多文本信息，因此一个好的分词策略，应该尽可能不出现 unknown token。</p> <ul><li>按照字符切分</li></ul> <p><img src="https://pic-1301573324.cos.ap-chengdu.myqcloud.com/20250913105324.png" alt="image.png"></p> <p>这种策略把文本切分成一个一个字符，这样只会产生一个非常小的词表，并且很少会出现词表外的 tokens</p> <p>但是从直觉上来看，字符本身并没有太大的意义，因此将文本切分为字符之后就会变得不容易理解。</p> <ul><li><strong>按照子词切分</strong></li></ul> <p>高频词直接保留，低频词被切分为更有意义的子词，例如 “annoyingly” 是一个低频词，可以切分为 &quot;annoying&quot; 和 &quot;ly&quot; 这两个子词不仅出现频率更高，而且词义可以得以保留</p> <p><img src="https://pic-1301573324.cos.ap-chengdu.myqcloud.com/20250913105510.png" alt="image.png"></p> <p>可以看到，“tokenization” 被切分为了 “token” 和 “ization”，不仅保留了语义，而且只用两个 token 就表示了一个长词。这种策略只用一个较小的词表就可以覆盖绝大部分文本，基本不会产生 unknown token。尤其对于土耳其语等黏着语，几乎所有的复杂长词都可以通过串联多个子词构成。</p> <h3 id="加载与保存分词器"><a href="#加载与保存分词器" class="header-anchor">#</a> 加载与保存分词器</h3> <p>分词器的加载与保存与模型相似，使用 <code>Tokenizer.from_pretrained()</code> 和 <code>Tokenizer.save_pretrained()</code> 函数。例如加载并保存 BERT 模型的分词器：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizer

tokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;bert-base-cased&quot;</span><span class="token punctuation">)</span>
tokenizer<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">&quot;./models/bert-base-cased/&quot;</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>同样地，在大部分情况下我们都应该使用 <code>AutoTokenizer</code> 来加载分词器：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;bert-base-cased&quot;</span><span class="token punctuation">)</span>
tokenizer<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">&quot;./models/bert-base-cased/&quot;</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>调用 <code>Tokenizer.save_pretrained()</code> 函数会在保存路径下创建三个文件：</p> <ul><li><code>special_tokens_map.json</code>：映射文件，里面包含 unknown token 等特殊字符的映射关系；</li> <li><code>tokenizer_config.json</code>：分词器配置文件，存储构建分词器需要的参数；</li> <li><code>vocab.txt</code>：词表，一行一个 token，行号就是对应的 token ID（从 0 开始）</li></ul> <h3 id="编码与解码文本"><a href="#编码与解码文本" class="header-anchor">#</a> 编码与解码文本</h3> <p>Encoding 过程包含两个步骤</p> <ol><li>分词：使用分词器按某种策略将文本切分为 tokens</li> <li>映射：将 tokens 转化为对应的 token IDs</li></ol> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;bert-base-cased&quot;</span><span class="token punctuation">)</span>

sequence <span class="token operator">=</span> <span class="token string">&quot;Using a Transformer network is simple&quot;</span>
tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>sequence<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>['using', 'a', 'transform', '##er', 'network', 'is', 'simple']
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以看到，BERT 分词器采用的是子词的分词策略，例如 “transformer” 会被切分为 “transform” 和 “##er”</p> <p>然后，我们通过 <code>convert_tokens_to_ids()</code> 将切分出的 tokens 转化成对应的 token IDs：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>ids<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>[7993, 170, 13809, 23763, 2443, 1110, 3014]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>我们还可以通过 <code>encode()</code> 函数将两个步骤合并，并且 <code>encode()</code> 会自动添加模型需要的特殊的 token，例如 BERT 分词器分别会在模型的首尾添加 <code>[CLS]</code> 和 <code>[SEP]</code></p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;bert-base-cased&quot;</span><span class="token punctuation">)</span>

sequence <span class="token operator">=</span> <span class="token string">&quot;Using a Transformer network is simple&quot;</span>
sequence_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>sequence<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>sequence_ids<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>[101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>其中 101 和 102 就是 <code>[CLS]</code> 和 <code>[SEP]</code> 对应的 tokens IDs</p> <p>在实际编码文本时，最常见的是直接使用分词器进行处理，这样不仅会返回分词后的 token IDs，还包含了模型需要的其他输入，例如 BERT 分词器还会在自动输入中添加 <code>token_type_ids</code> 和 <code>attention_mask</code></p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;bert-base-cased&quot;</span><span class="token punctuation">)</span>
tokenized_text <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">&quot;Using a Transformer network is simple&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokenized_text<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>文本解码 (Decoding) 与编码相反，负责将 token IDs 转换回原来的字符串。注意，解码过程不是简单地将 token IDs 映射回 tokens，还需要合并那些被分为多个 token 的单词。下面我们通过 <code>decode()</code> 函数解码前面生成的 token IDs：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;bert-base-cased&quot;</span><span class="token punctuation">)</span>

decoded_string <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">7993</span><span class="token punctuation">,</span> <span class="token number">170</span><span class="token punctuation">,</span> <span class="token number">11303</span><span class="token punctuation">,</span> <span class="token number">1200</span><span class="token punctuation">,</span> <span class="token number">2443</span><span class="token punctuation">,</span> <span class="token number">1110</span><span class="token punctuation">,</span> <span class="token number">3014</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>decoded_string<span class="token punctuation">)</span>

decoded_string <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">7993</span><span class="token punctuation">,</span> <span class="token number">170</span><span class="token punctuation">,</span> <span class="token number">13809</span><span class="token punctuation">,</span> <span class="token number">23763</span><span class="token punctuation">,</span> <span class="token number">2443</span><span class="token punctuation">,</span> <span class="token number">1110</span><span class="token punctuation">,</span> <span class="token number">3014</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>decoded_string<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>Using a transformer network is simple
[CLS] Using a Transformer network is simple [SEP]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>解码文本是一个重要的步骤，在进行文本生成、翻译或者摘要等 Seq2Seq (Sequence-to-Sequence) 任务时都会调用这一函数。</p> <h2 id="处理多段文本"><a href="#处理多段文本" class="header-anchor">#</a> 处理多段文本</h2> <p>我们往往会同时处理多段文本，而且模型只接受 batch 数据作为输入，即使只有一段文本，也需要将它组成一个只包含一个样本的 batch</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForSequenceClassification

checkpoint <span class="token operator">=</span> <span class="token string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

sequence <span class="token operator">=</span> <span class="token string">&quot;I've been waiting for a HuggingFace course my whole life.&quot;</span>

tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>sequence<span class="token punctuation">)</span>
ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
<span class="token comment"># input_ids = torch.tensor(ids), This line will fail.</span>
input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>ids<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Input IDs:\n&quot;</span><span class="token punctuation">,</span> input_ids<span class="token punctuation">)</span>

output <span class="token operator">=</span> model<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Logits:\n&quot;</span><span class="token punctuation">,</span> output<span class="token punctuation">.</span>logits<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>Input IDs: 
tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,
          2026,  2878,  2166,  1012]])
Logits: 
tensor([[-2.7276,  2.8789]], grad_fn=&lt;AddmmBackward0&gt;)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这里我们通过 <code>[ids]</code> 构建了一个只包含一段文本的 batch，更常见的是送入包含多段文本的 batch：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>batched_ids <span class="token operator">=</span> <span class="token punctuation">[</span>ids<span class="token punctuation">,</span> ids<span class="token punctuation">,</span> ids<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>实际场景中，我们应该直接使用分词器对文本进行处理</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForSequenceClassification

checkpoint <span class="token operator">=</span> <span class="token string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

sequence <span class="token operator">=</span> <span class="token string">&quot;I've been waiting for a HuggingFace course my whole life.&quot;</span>

tokenized_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>sequence<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Inputs Keys:\n&quot;</span><span class="token punctuation">,</span> tokenized_inputs<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\nInput IDs:\n&quot;</span><span class="token punctuation">,</span> tokenized_inputs<span class="token punctuation">[</span><span class="token string">&quot;input_ids&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>tokenized_inputs<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\nLogits:\n&quot;</span><span class="token punctuation">,</span> output<span class="token punctuation">.</span>logits<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>Inputs Keys:
 dict_keys(['input_ids', 'attention_mask'])

Input IDs:
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])

Logits:
tensor([[-1.5607,  1.6123]], grad_fn=&lt;AddmmBackward0&gt;)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>可以看到，分词器输出的结果中不仅包含 token IDs（ <code>input_ids</code> ），还会包含模型需要的其他输入项。前面我们之所以只输入 token IDs 模型也能正常运行，是因为它自动地补全了其他的输入项，例如 <code>attention_mask</code> 等</p> <h3 id="padding-操作"><a href="#padding-操作" class="header-anchor">#</a> Padding 操作</h3> <p>按批输入多段文本产生的一个直接问题就是：batch 中的文本有长有短，而输入张量必须是严格的二维矩形，维度为 <span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>batch size</mtext><mo separator="true">,</mo><mtext>sequence length</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{batch size}, \text{sequence length})</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord text"><span class="mord">batch size</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">sequence length</span></span><span class="mclose">)</span></span></span></span></span>，即每一段文本编码后的 token IDs 数量必须一样多。例如下面的 ID 列表是无法转换为张量的：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>batched_ids <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>我们需要通过 Padding 操作，在短序列的结尾填充特殊的 padding token，使得 batch 中所有的序列都具有相同的长度</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>padding_id <span class="token operator">=</span> <span class="token number">100</span>

batched_ids <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> padding_id<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>模型的 padding token ID 可以通过其分词器的 <code>pad_token_id</code> 属性获得。下面我们尝试将两段文本分别以独立以及 batch 的形式送入到模型中：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForSequenceClassification

checkpoint <span class="token operator">=</span> <span class="token string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

sequence1_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
sequence2_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
batched_ids <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> tokenizer<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>sequence1_ids<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>logits<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>sequence2_ids<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>logits<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>batched_ids<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>logits<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>tensor([[ 1.5694, -1.3895]], grad_fn=&lt;AddmmBackward0&gt;)
tensor([[ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward0&gt;)
tensor([[ 1.5694, -1.3895],
        [ 1.3374, -1.2163]], grad_fn=&lt;AddmmBackward0&gt;)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>我们发现了，使用 padding token 填充的序列的结果竟然与其单独送入模型时不同！</p> <p>这是因为模型默认会编码输入序列中的所有 token 以建模完整的上下文，因此这里会将填充的 padding token 也一同编码进去，从而生成不同的语义表示。</p> <p>因此，在进行 Padding 操作时，我们必须明确告知模型哪些 token 是我们填充的，它们不应该参与编码。这就需要使用到 Attention Mask 了</p> <h3 id="attention-mask"><a href="#attention-mask" class="header-anchor">#</a> Attention Mask</h3> <p>Attention Mask 是一个尺寸与 input IDs 完全相同，且仅由 0 和 1 组成的张量，0 表示对应位置的 token 是填充符，不参与计算。当然，一些特殊的模型结构也会借助 Attention Mask 来遮蔽掉指定的 tokens</p> <p>对于上面的例子，如果我们通过 attention_mask 标出填充的 padding token 的位置，计算结果就不会有问题了</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForSequenceClassification

checkpoint <span class="token operator">=</span> <span class="token string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

sequence1_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
sequence2_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
batched_ids <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> tokenizer<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>
batched_attention_masks <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>sequence1_ids<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>logits<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>sequence2_ids<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>logits<span class="token punctuation">)</span>
outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>
    torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>batched_ids<span class="token punctuation">)</span><span class="token punctuation">,</span> 
    attention_mask<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>batched_attention_masks<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>logits<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>tensor([[ 1.5694, -1.3895]], grad_fn=&lt;AddmmBackward0&gt;)
tensor([[ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward0&gt;)
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward0&gt;)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>所以说，在实际使用的时候，我们应该直接使用分词对文本进行处理，它不仅会向 token 序列中添加模型需要的特殊字符（例如 <code>[CLS],[SEP]</code>），还会自动生成对应的 Attention Mask。</p> <p>目前大部分 Transformer 模型只能接受长度不超过 512 或 1024 的 token 序列，因此对于长序列，有三种处理办法：</p> <ol><li>使用一个支持长文的 Transformer 模型，例如 Longformer 和 LED</li> <li>设定最大长度，然后截断输入序列：<code>sequence = sequence[:max_squence_lengeth]</code></li> <li>将长文切片为短的文本块，然后对每一个 chunk 编码</li></ol> <h3 id="直接使用分词器"><a href="#直接使用分词器" class="header-anchor">#</a> 直接使用分词器</h3> <p>正如上面所说，在实际使用时，我们应该直接使用分词器来进行分词</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

checkpoint <span class="token operator">=</span> <span class="token string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

sequences <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">&quot;I've been waiting for a HuggingFace course my whole life.&quot;</span><span class="token punctuation">,</span> 
    <span class="token string">&quot;So have I!&quot;</span>
<span class="token punctuation">]</span>

model_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>sequences<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model_inputs<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>{'input_ids': [
    [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 
    [101, 2061, 2031, 1045, 999, 102]], 
 'attention_mask': [
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
    [1, 1, 1, 1, 1, 1]]
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>可以看到，分词器的输出包含了模型需要的所有输入项</p> <p>对于 DistilBERT 模型，就是 <code>input_ids</code> 和 <code>attention_mask</code></p> <p>Padding 操作通过 <code>padding</code> 参数来控制</p> <ul><li><code>padding=&quot;longest&quot;</code> 表示将序列填充到当前 batch 中最长序列的长度</li> <li><code>padding=&quot;max_lenth&quot;</code> 将所有序列填充到模型所能接受的最大长度，例如 BERT 模型就是 512</li></ul> <p><strong>截断操作</strong> 通过 <code>truncation</code> 参数来控制，如果 <code>truncation=True</code>，那么大于模型最大接受长度的序列会被阶段，例如对于 BERT 模型就会截断长度超过 512 的序列，此外，也可以通过 <code>max_length</code> 参数来控制截断长度</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

checkpoint <span class="token operator">=</span> <span class="token string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

sequences <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">&quot;I've been waiting for a HuggingFace course my whole life.&quot;</span><span class="token punctuation">,</span> 
    <span class="token string">&quot;So have I!&quot;</span>
<span class="token punctuation">]</span>

model_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model_inputs<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>{'input_ids': [
    [101, 1045, 1005, 2310, 2042, 3403, 2005, 102], 
    [101, 2061, 2031, 1045, 999, 102]], 
 'attention_mask': [
    [1, 1, 1, 1, 1, 1, 1, 1], 
    [1, 1, 1, 1, 1, 1]]
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>分词器还可以通过 <code>return_tensors</code> 参数指定返回的张量格式，设 <code>pt</code> 返回 Pytorch 张量，<code>tf</code> 返回 TensorFlow 张量，<code>np</code> 则返回 NumPy 数组，例如：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

checkpoint <span class="token operator">=</span> <span class="token string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

sequences <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">&quot;I've been waiting for a HuggingFace course my whole life.&quot;</span><span class="token punctuation">,</span> 
    <span class="token string">&quot;So have I!&quot;</span>
<span class="token punctuation">]</span>

model_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model_inputs<span class="token punctuation">)</span>

model_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">&quot;np&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model_inputs<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>{'input_ids': tensor([
    [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
      2607,  2026,  2878,  2166,  1012,   102],
    [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,
         0,     0,     0,     0,     0,     0]]), 
 'attention_mask': tensor([
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
}

{'input_ids': array([
    [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
     12172,  2607,  2026,  2878,  2166,  1012,   102],
    [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,
         0,     0,     0,     0,     0,     0,     0]]), 
 'attention_mask': array([
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>实际使用分词器时，我们通常会同时进行 padding 操作和截断操作，并设置返回格式为 Pytorch 张量，这样可以直接将分词结果送入模型</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForSequenceClassification

checkpoint <span class="token operator">=</span> <span class="token string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

sequences <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">&quot;I've been waiting for a HuggingFace course my whole life.&quot;</span><span class="token punctuation">,</span> 
    <span class="token string">&quot;So have I!&quot;</span>
<span class="token punctuation">]</span>

tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>tokens<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>logits<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>{'input_ids': tensor([
    [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
      2607,  2026,  2878,  2166,  1012,   102],
    [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,
         0,     0,     0,     0,     0,     0]]), 
 'attention_mask': tensor([
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}

tensor([[-1.5607,  1.6123],
        [-3.6183,  3.9137]], grad_fn=&lt;AddmmBackward0&gt;)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>在设置了 <code>padding=True, truncation=True</code> 设置下，同一个 batch 中的序列都会 padding 到相同的长度，并且大于模型最大接受长度的序列会被自动截断</p> <h3 id="编码句子对"><a href="#编码句子对" class="header-anchor">#</a> 编码句子对</h3> <p>对于 BERT 等包含“句子对”预训练任务的模型，他们的分词器都支持对句子对进行编码，例如：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">&quot;This is the first sentence.&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;This is the second one.&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>

tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_ids_to_tokens<span class="token punctuation">(</span>inputs<span class="token punctuation">[</span><span class="token string">&quot;input_ids&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}

['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>通过输出我们发现分词器会用 <code>[SEP]</code> token 拼接两个句子，这是 BERT 模型预期的 &quot;句子对&quot; 输入格式</p> <p>返回结果中除了前面我们介绍过的 <code>inputs_ids</code> 和 <code>attention_mask</code> 此外，还包含了一个 <code>token_type_ids</code> 项，用于标记那些 token 属于第一个句子，哪些属于第二个句子</p> <p>我们可以看到第一个句子 <code>[CLS] sentence1 [SEP]</code> 所有 token 的 token ID 都为 0，而第二个句子 <code>sentence2 [SEP]</code> 对应的 token type ID 都为 1</p> <p>实际使用时，我们不需要去关注编码结果中是否包含 <code>token_type_ids</code> 项，分词器会根据 checkpoint 自动调整输出格式</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

sentence1_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;First sentence.&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;This is the second sentence.&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;Third one.&quot;</span><span class="token punctuation">]</span>
sentence2_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;First sentence is short.&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;The second sentence is very very very long.&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;ok.&quot;</span><span class="token punctuation">]</span>

tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>
    sentence1_list<span class="token punctuation">,</span>
    sentence2_list<span class="token punctuation">,</span>
    padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span>
<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokens<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>{'input_ids': tensor([
        [ 101, 2034, 6251, 1012,  102, 2034, 6251, 2003, 2460, 1012,  102,    0,
            0,    0,    0,    0,    0,    0],
        [ 101, 2023, 2003, 1996, 2117, 6251, 1012,  102, 1996, 2117, 6251, 2003,
         2200, 2200, 2200, 2146, 1012,  102],
        [ 101, 2353, 2028, 1012,  102, 7929, 1012,  102,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0]]), 
 'token_type_ids': tensor([
        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
 'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
}
torch.Size([3, 18])
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>可以看到分词器成功地输出了形式为 <code>[CLS] sentence1 [SEP] sentence2 [SEP]</code> 的 token 序列，并且三个序列都 padding 到了相同的长度</p> <h2 id="添加-token"><a href="#添加-token" class="header-anchor">#</a> 添加 Token</h2> <p>实际操作中，我们还经常会遇到输入中需要包含特殊标记符的情况，例如使用 <code>[ENT_START]</code> 和 <code>[ENT_END]</code> 标记出文本中的实体。由于这些自定义 token 并不在预训练模型原来的词表中，因此直接运用分词器处理就会出现问题。</p> <p>例如直接使用 BERT 分词器处理下面的句子：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

sentence <span class="token operator">=</span> <span class="token string">'Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.'</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>['two', '[', 'en', '#t', '_','start', ']', 'cars', '[', 'en', '#t', '_', 'end', ']', 'collided', 'i
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>由于分词器无法识别 <code>[ENT_START]</code> 和 <code>[ENT_END]</code>，因此将它们都当作未知字符处理，例如 <code>[ENT_END]</code>被切分成了六个 token。</p> <p>此外，一些领域的专业词汇，例如使用多个词语的缩写拼接而成的医学术语，同样也不在模型的词表中，因此也会出现上面的问题。此时我们就需要将这些新 token 添加到模型的词表中，让分词器与模型可以识别并处理这些 token。</p> <h3 id="添加新-token"><a href="#添加新-token" class="header-anchor">#</a> 添加新 token</h3> <p>Transformers 库提供了两种方式来添加新的 token，分别是</p> <ul><li><code>add_tokens()</code> 添加普通的 token：参数是新的 token 列表，如果 token 不在列表中，就会被添加到词表的最后</li></ul> <div class="language-python line-numbers-mode"><pre class="language-python"><code>checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
    
num_added_toks <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>add_tokens<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&quot;new_token1&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;my_new-token2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;We have added&quot;</span><span class="token punctuation">,</span> num_added_toks<span class="token punctuation">,</span> <span class="token string">&quot;tokens&quot;</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>We have added 2 tokens
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>为了防止 token 已经包含在词表中，我们还可以预先对新 token 列表进行过滤</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>new_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;new_token1&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;my_new-token2&quot;</span><span class="token punctuation">]</span>
new_tokens <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>new_tokens<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token builtin">set</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
tokenizer<span class="token punctuation">.</span>add_tokens<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>new_tokens<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li><code>add_special_tokens()</code> 添加特殊 token：参数是包含特殊 token 的字典，键值只能从 <code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>, <code>additional_special_tokens</code> 中选择。同样地，如果 token 不在词表中，就会被添加到词表的最后。添加后，还可以通过特殊属性来访问这些 token，例如 <code>tokenizer.cls_token</code> 就指向 <code>cls token</code>。</li></ul> <div class="language-python line-numbers-mode"><pre class="language-python"><code>checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
    
special_tokens_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&quot;cls_token&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;[MY_CLS]&quot;</span><span class="token punctuation">}</span>
    
num_added_toks <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>add_special_tokens<span class="token punctuation">(</span>special_tokens_dict<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;We have added&quot;</span><span class="token punctuation">,</span> num_added_toks<span class="token punctuation">,</span> <span class="token string">&quot;tokens&quot;</span><span class="token punctuation">)</span>
    
<span class="token keyword">assert</span> tokenizer<span class="token punctuation">.</span>cls_token <span class="token operator">==</span> <span class="token string">&quot;[MY_CLS]&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>We have added 1 tokens
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>我们也可以使用 <code>add_tokens()</code> 添加特殊的 token，只需要额外设置参数 <code>special_tokens=True</code></p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
    
num_added_toks <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>add_tokens<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&quot;[NEW_tok1]&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;[NEW_tok2]&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
num_added_toks <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>add_tokens<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&quot;[NEW_tok3]&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;[NEW_tok4]&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;We have added&quot;</span><span class="token punctuation">,</span> num_added_toks<span class="token punctuation">,</span> <span class="token string">&quot;tokens&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span><span class="token string">'[NEW_tok1] Hello [NEW_tok2] [NEW_tok3] World [NEW_tok4]!'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>We have added 2 tokens
['[new_tok1]', 'hello', '[new_tok2]', '[NEW_tok3]', 'world', '[NEW_tok4]', '!']
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>对于前面的例子，很明显实体标记符 <code>[ENT_START]</code> 和 <code>[ENT_END]</code> 属于特殊的 token，因此按添加特殊 token 的方式进行</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

num_added_toks <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>add_tokens<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'[ENT_START]'</span><span class="token punctuation">,</span> <span class="token string">'[ENT_END]'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment"># num_added_toks = tokenizer.add_special_tokens({'additional_special_tokens': ['[ENT_START]', '[ENT_END]']})</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;We have added&quot;</span><span class="token punctuation">,</span> num_added_toks<span class="token punctuation">,</span> <span class="token string">&quot;tokens&quot;</span><span class="token punctuation">)</span>

sentence <span class="token operator">=</span> <span class="token string">'Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.'</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>We have added 2 tokens
['two', '[ENT_START]', 'cars', '[ENT_END]', 'collided', 'in', 'a', '[ENT_START]', 'tunnel', '[ENT_END]', 'this', 'morning', '.']
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>可以看到，分词器成功地将 <code>[ENT_START]</code> 和 <code>[ENT_END]</code> 识别为 token，并且保持大写</p> <h3 id="调整-embedding-矩阵"><a href="#调整-embedding-矩阵" class="header-anchor">#</a> 调整 embedding 矩阵</h3> <p>向词表中添加新 token 后，必须重置模型 embedding 矩阵的大小，也就是向矩阵中添加新 token 对应的 embedding，这样模型才能正常工作，讲 token 映射到对应的 embedding</p> <p>调整 embedding 矩阵通过 <code>resize_token_embeddings()</code> 函数实现，例如对于前面的例子</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModel

checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'vocabulary size:'</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">)</span><span class="token punctuation">)</span>
num_added_toks <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>add_tokens<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'[ENT_START]'</span><span class="token punctuation">,</span> <span class="token string">'[ENT_END]'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;After we add&quot;</span><span class="token punctuation">,</span> num_added_toks<span class="token punctuation">,</span> <span class="token string">&quot;tokens&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'vocabulary size:'</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>resize_token_embeddings<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># Randomly generated matrix</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>vocabulary size: 30522
After we add 2 tokens
vocabulary size: 30524
torch.Size([30524, 768])

tensor([[-0.0325, -0.0224,  0.0044,  ..., -0.0088, -0.0078, -0.0110],
        [-0.0005, -0.0167, -0.0009,  ...,  0.0110, -0.0282, -0.0013]],
       grad_fn=&lt;SliceBackward0&gt;)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>可以看到，在添加 <code>[ENT_START]</code> 和 <code>[ENT_END]</code> 之后，分词器的词表大小从 30522 增加到了 30524，模型 embedding 矩阵的大小也调整到了 <span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>30524</mn><mo>×</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">30524\times 768</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mord">0</span><span class="mord">5</span><span class="mord">2</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">7</span><span class="mord">6</span><span class="mord">8</span></span></span></span></span></p> <p>在默认情况下，新添加的 token 的embedding 是随机初始化的</p> <h2 id="token-embedding-初始化"><a href="#token-embedding-初始化" class="header-anchor">#</a> Token embedding 初始化</h2> <p>如果有充分的语料对模型进行微调或者继续预训练，那么将新添加 token 初始化为随机向量没什么问题。但是如果训练语料较少，甚至是只有很少语料的 few-shot learning 场景下，这种做法就存在问题。研究表明，在训练数据不够多的情况下，这些新添加 token 的 embedding 只会在初始值附近小幅波动。换句话说，即使经过训练，它们的值事实上还是随机的。</p> <h3 id="直接赋值"><a href="#直接赋值" class="header-anchor">#</a> 直接赋值</h3> <p>因此，在很多情况下，我们需要手工初始化新添加 token 的embeding</p> <p>例如，我们将上面例子中两个新 token 的embedding 都初始化为全 0 向量</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch

<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=&lt;SliceBackward0&gt;)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>初始化 embedding 的过程并不可导，因此这里通过 <code>torch.no_grad()</code> 来暂停梯度的计算</p> <p>在现实场景中，更为常见的做法是使用已有 token 的 embedding 来初始化新 token，例如，对于上面的例子，我们可以将 <code>[ENT_START]</code> 和 <code>[ENT_END]</code> 的值都初始化为 &quot;entity&quot; token 对应的 embedding</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch

token_id <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">(</span><span class="token string">'entity'</span><span class="token punctuation">)</span>
token_embedding <span class="token operator">=</span> model<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">[</span>token_id<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>token_id<span class="token punctuation">)</span>

<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_added_toks<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token operator">-</span>i<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> token_embedding<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>9178
tensor([[-0.0039, -0.0131, -0.0946,  ..., -0.0223,  0.0107, -0.0419],
        [-0.0039, -0.0131, -0.0946,  ..., -0.0223,  0.0107, -0.0419]],
       grad_fn=&lt;SliceBackward0&gt;)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>因为 token ID 就是 token 在 embedding 矩阵中的索引，因此这里我们直接通过 <code>weight[token_id]</code> 取出“entity”对应的 embedding。</p> <p>可以看到最终结果符合我们的预期，<code>[ENT_START]</code> 和 <code>[ENT_END]</code> 都被初始化为相同的 embedding</p> <h3 id="初始化已有-token-的值"><a href="#初始化已有-token-的值" class="header-anchor">#</a> 初始化已有 token 的值</h3> <p>更为高级的做法是根据新添加 token 的语义来进行初始化。例如将值初始化为 token 语义描述中所有 token 的平均值，假设新 token <span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">t_i</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>的语义描述为<span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mn>2</mn></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{i,1}, w_{i,2}, \ldots, w_{i,n}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></span>，那么初始化 <span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">t_i</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>的 embedding 为：</p> <section><div class="vuepress-eqn"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">E</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="bold-italic">E</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
\boldsymbol{E}(t_i) = \frac{1}{n} \sum_{j=1}^{n} \boldsymbol{E}(w_{i,j})
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.05451em;">E</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0651740000000007em;vertical-align:-1.4137769999999998em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000007em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.05451em;">E</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></section><p>这里<span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">E</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{E}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.05451em;">E</span></span></span></span></span></span></span> 表示预训练模型的 embedding 矩阵。对于上面的例子，我们可以分别为 <code>[ENT_START]</code> 和 <code>[ENT_END]</code> 编写对应的描述，然后再对它们的值进行初始化：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>descriptions <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'start of entity'</span><span class="token punctuation">,</span> <span class="token string">'end of entity'</span><span class="token punctuation">]</span>

<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> token <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">reversed</span><span class="token punctuation">(</span>descriptions<span class="token punctuation">)</span><span class="token punctuation">,</span> start<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        tokenized <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>token<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>tokenized<span class="token punctuation">)</span>
        tokenized_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">(</span>tokenized<span class="token punctuation">)</span>
        new_embedding <span class="token operator">=</span> model<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">[</span>tokenized_ids<span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token operator">-</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> new_embedding<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>['end', 'of', 'entity']
['start', 'of', 'entity']
tensor([[-0.0340, -0.0144, -0.0441,  ..., -0.0016,  0.0318, -0.0151],
        [-0.0060, -0.0202, -0.0312,  ..., -0.0084,  0.0193, -0.0296]],
       grad_fn=&lt;SliceBackward0&gt;)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以看到，这里成功地将 <code>[ENT_START]</code> 的 embedding 初始化为“start”、“of”、“entity”三个 token 的平均值，将 <code>[ENT_END]</code> 初始化为“end”、“of”、“entity”的平均值。</p></div></div> <!----> <div class="page-edit"><!----> <!----> <!----></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/a46255/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">一个简单的 PyTorch 图像分类器</div></a> <a href="/pages/fa9210/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">计算机网络笔记</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/a46255/" class="prev">一个简单的 PyTorch 图像分类器</a></span> <span class="next"><a href="/pages/fa9210/">计算机网络笔记</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/pages/c9188a/"><div>
            打破信息差，一条学校不教的计算机学习之路
            <!----></div></a> <span class="date">09-18</span></dt></dl><dl><dd>02</dd> <dt><a href="/pages/a3873d/"><div>
            ICPC2025Online2 E Zero
            <!----></div></a> <span class="date">09-18</span></dt></dl><dl><dd>03</dd> <dt><a href="/pages/0ded53/"><div>
            大二总结
            <!----></div></a> <span class="date">09-03</span></dt></dl> <dl><dd></dd> <dt><a href="/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="mailto:1485868106@qq.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/chengyiwei" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="https://music.163.com/#/user/home?id=1413383674" title="听音乐" target="_blank" class="iconfont icon-erji"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2024-2025
    <span>Martian148 | <a href="https://github.com/xugaoyi/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><div></div></div></div>
    <script src="/assets/js/app.627fef7e.js" defer></script><script src="/assets/js/2.30da42f9.js" defer></script><script src="/assets/js/25.6fcc0217.js" defer></script>
  </body>
</html>
